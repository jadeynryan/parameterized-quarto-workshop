[
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Data Cleaning: Exercises",
    "section": "",
    "text": "Recommended: Complete these exercises in the dedicated Posit Cloud work space, which comes with\n\nall packages pre-installed, and\nan Rmarkdown document to fill in.\n\nIt will be helpful to peek here to verify that your tables generated in Posit Cloud match the desired output (or even to peek at the Code Solution if your cleaned tables donâ€™t look right).\nClick here to enter the R/Medicine Posit cloud work space\nOtherwise: Follow along this document, work on your personal computer, and challenge yourself not to peek at the code solutions until you have completed the exercise."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles of Data Cleaning",
    "section": "",
    "text": "ðŸ—“ June 6, 2023 | 9:30am - 1:00pm EDT\nðŸ¨ Virtual\nðŸ’¥ FREE workshop registration"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Principles of Data Cleaning",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nTo import Excel files with (common, messy) data problems.\nTo address and clean common messy data problems in each variable\nTo address and clean data with more complex meta-problems, like pivoting to long format for data analysis, dealing with multi-column headers, color-coded data (gah!), and un-pivoting pivot tables into tidy data."
  },
  {
    "objectID": "index.html#is-this-course-for-me",
    "href": "index.html#is-this-course-for-me",
    "title": "Principles of Data Cleaning",
    "section": "Is this course for me?",
    "text": "Is this course for me?\nIf your answer to any of the following questions is â€œyesâ€, then this is the right workshop for you.\n\nDo you frequently receive untidy data for analysis in Excel spreadsheets?\nDoes this drive you slightly batty?\nDo you want to learn how to make your life easier with a suite of data cleaning tools?\n\nThe workshop is designed for those with some experience in R. It will be assumed that participants can perform basic data manipulation. Experience with the {tidyverse} and the %&gt;%/|&gt; operator is a plus, but not required."
  },
  {
    "objectID": "index.html#prework",
    "href": "index.html#prework",
    "title": "Principles of Data Cleaning",
    "section": "Prework",
    "text": "Prework\nThis workshop will largely be conducted in the Posit Cloud environment. Please create a login to the Posit Cloud instance of this workshop here:\nPosit Cloud Medical Cleaning Workshop.\nIf you will not be using the Posit Cloud instance, you can just watch along as the instructors teach.\nIf you would like to try this out on your own computer, please have the following installed and configured on your machine.\n\nRecent version of R\nRecent version of RStudio\nRecent version of packages used in workshop.\ninstll_pkgs &lt;- c(\"tidyverse\", \"tidyxl\", \"unpivotr\", \"readxl\", \"openxlsx\", \"janitor\", \"gtsummary\", \"here\", \"padr\")\ninstall.packages(instll_pkgs)\nEnsure you can knit R markdown documents\nOpen RStudio and create a new Rmarkdown document\nSave the document and check you are able to knit it.\nEnsure you can import data from a Microsoft Excel spreadsheet to R\n\nSave a a Microsoft Excel spreadsheet in a known folder on your computer.\nOpen RStudio install/load (library) the packages {readxl} (link) and {openxlsx} (link)\nImport the data from the spreadsheet into R using the path to the correct folder with either package."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons 4.0 International."
  },
  {
    "objectID": "slides/00-setup.html",
    "href": "slides/00-setup.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” ggplot2 3.3.6     âœ” purrr   0.3.5\nâœ” tibble  3.1.8     âœ” dplyr   1.1.0\nâœ” tidyr   1.2.1     âœ” stringr 1.4.1\nâœ” readr   2.1.3     âœ” forcats 0.5.2\n\n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\n\n\n\n\nThis work is licensed under Creative Commons Zero v1.0 Universal.\n\n\n\n\n\nCrystal Lewis\n Shannon Pileggi\n\n\nPeter Higgins\n\nDuncan Garmonsway\n\n\n\n\n\n\n\n Install recent R release\nCurrent version 4.2.1\n Install RStudio\nI am on version 2022.07.1+554 \n Install packages\ninstall.packages(c(\"gtreg\", \"gtsummary\", \"tidyverse\", \"labelled\", \"usethis\"))\n Ensure you can knit Rmarkdown files\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n09:00 - 09:50\nSection 1\n\n\n09:50 - 10:00\nBreak\n\n\n10:00 - 10:50\nSection 2\n\n\n10:50 - 11:00\nBreak\n\n\n11:00 - 12:00\nSection 3\n\n\n\n\nPlease add any questions to the public Zoom chat. These may be answered in the moment or addressed at the end depending on context."
  },
  {
    "objectID": "slides/01-crystal.html#slide-1-1",
    "href": "slides/01-crystal.html#slide-1-1",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/01-crystal.html#slide-2-1",
    "href": "slides/01-crystal.html#slide-2-1",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/02-shannon.html#slide-1-1",
    "href": "slides/02-shannon.html#slide-1-1",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/02-shannon.html#slide-2-1",
    "href": "slides/02-shannon.html#slide-2-1",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/03-peter.html#slide-1-1",
    "href": "slides/03-peter.html#slide-1-1",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/03-peter.html#slide-2-1",
    "href": "slides/03-peter.html#slide-2-1",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/04-duncan.html#slide-1-1",
    "href": "slides/04-duncan.html#slide-1-1",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/04-duncan.html#slide-2-1",
    "href": "slides/04-duncan.html#slide-2-1",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/05-thank-you.html",
    "href": "slides/05-thank-you.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "ðŸ™ Click here to submit workshop feedback"
  },
  {
    "objectID": "slides/index.html#licensing",
    "href": "slides/index.html#licensing",
    "title": "Data Cleaning",
    "section": "Licensing",
    "text": "Licensing\n\nThis work is licensed under Creative Commons Zero v1.0 Universal."
  },
  {
    "objectID": "slides/index.html#instructors",
    "href": "slides/index.html#instructors",
    "title": "Data Cleaning",
    "section": "Instructors",
    "text": "Instructors\n\n\nCrystal Lewis\n\n\nShannon Pileggi\n\n\nPeter Higgins"
  },
  {
    "objectID": "slides/index.html#checklist",
    "href": "slides/index.html#checklist",
    "title": "Data Cleaning",
    "section": "Checklist",
    "text": "Checklist\n\n Install recent R release\nCurrent version 4.3.0\n Install RStudio\nI am on version 2022.07.1+554 \n Install packages\n# install.packages(c(\"gtreg\", \"gtsummary\", \"tidyverse\", \"labelled\", \"usethis\"))\n Ensure you can knit Rmarkdown files"
  },
  {
    "objectID": "slides/index.html#schedule",
    "href": "slides/index.html#schedule",
    "title": "Data Cleaning",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n11:00 - 11:50\nCrystal Lewis (Data management best practices)\n\n\n11:50 - 12:00\nBreak\n\n\n12:00 - 12:50\nShannon Pileggi (Data cleaning fundamentals)\n\n\n12:50 - 12:00\nBreak\n\n\n01:00 - 02:00\nPeter Higgins (Data wrangling & reshaping)\n\n\n\n\nPlease add any questions to the public Zoom chat. These may be answered in the moment or addressed at the end depending on context."
  },
  {
    "objectID": "slides/index.html#slide-1",
    "href": "slides/index.html#slide-1",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/index.html#slide-2",
    "href": "slides/index.html#slide-2",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/index.html#slide-1-1",
    "href": "slides/index.html#slide-1-1",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/index.html#slide-2-1",
    "href": "slides/index.html#slide-2-1",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/index.html#slide-1-2",
    "href": "slides/index.html#slide-1-2",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/index.html#slide-2-2",
    "href": "slides/index.html#slide-2-2",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/index.html#slide-1-3",
    "href": "slides/index.html#slide-1-3",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/index.html#slide-2-3",
    "href": "slides/index.html#slide-2-3",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/index.html#slide-1-4",
    "href": "slides/index.html#slide-1-4",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/index.html#slide-2-4",
    "href": "slides/index.html#slide-2-4",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/index.html#slide-1-5",
    "href": "slides/index.html#slide-1-5",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/index.html#slide-2-5",
    "href": "slides/index.html#slide-2-5",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/index.html#slide-1-6",
    "href": "slides/index.html#slide-1-6",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/index.html#slide-2-6",
    "href": "slides/index.html#slide-2-6",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/index.html#slide-1-7",
    "href": "slides/index.html#slide-1-7",
    "title": "Data Cleaning",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/index.html#slide-2-7",
    "href": "slides/index.html#slide-2-7",
    "title": "Data Cleaning",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "slides/index.html#thank-you",
    "href": "slides/index.html#thank-you",
    "title": "Data Cleaning",
    "section": "Thank you!",
    "text": "Thank you!\nðŸ™ Click here to submit workshop feedback\n\n\n\nData Cleaning"
  },
  {
    "objectID": "slidespage.html",
    "href": "slidespage.html",
    "title": "Slides",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "slides/03-peter.html#is-it-the-patient",
    "href": "slides/03-peter.html#is-it-the-patient",
    "title": "Data Cleaning",
    "section": "Is it the Patient?",
    "text": "Is it the Patient?"
  },
  {
    "objectID": "slides/03-peter.html#is-it-the-visitencounter",
    "href": "slides/03-peter.html#is-it-the-visitencounter",
    "title": "Data Cleaning",
    "section": "Is it the Visit/Encounter?",
    "text": "Is it the Visit/Encounter?"
  },
  {
    "objectID": "slides/03-peter.html#pivoting-longer-common",
    "href": "slides/03-peter.html#pivoting-longer-common",
    "title": "Data Cleaning",
    "section": "Pivoting Longer (common)",
    "text": "Pivoting Longer (common)\n\nVisit Dates/Measures\nYour Turn (Exercise)"
  },
  {
    "objectID": "slides/03-peter.html#pivoting-wider",
    "href": "slides/03-peter.html#pivoting-wider",
    "title": "Data Cleaning",
    "section": "Pivoting Wider",
    "text": "Pivoting Wider"
  },
  {
    "objectID": "slides/03-peter.html#padding-unobserved-dates-weekends",
    "href": "slides/03-peter.html#padding-unobserved-dates-weekends",
    "title": "Data Cleaning",
    "section": "Padding unobserved dates (weekends?)",
    "text": "Padding unobserved dates (weekends?)"
  },
  {
    "objectID": "slides/03-peter.html#padding-unovserved-times",
    "href": "slides/03-peter.html#padding-unovserved-times",
    "title": "Data Cleaning",
    "section": "Padding unovserved times",
    "text": "Padding unovserved times"
  },
  {
    "objectID": "slides/03-peter.html#local-demographics-with-cdc-svi-data-one-to-one",
    "href": "slides/03-peter.html#local-demographics-with-cdc-svi-data-one-to-one",
    "title": "Data Cleaning",
    "section": "Local Demographics with CDC SVI data (one to one)",
    "text": "Local Demographics with CDC SVI data (one to one)"
  },
  {
    "objectID": "slides/03-peter.html#patient-demographics-with-lab-results-one-to-many",
    "href": "slides/03-peter.html#patient-demographics-with-lab-results-one-to-many",
    "title": "Data Cleaning",
    "section": "Patient Demographics with Lab results (one to many)",
    "text": "Patient Demographics with Lab results (one to many)"
  },
  {
    "objectID": "slides/index.html#is-it-the-patient",
    "href": "slides/index.html#is-it-the-patient",
    "title": "Data Cleaning",
    "section": "Is it the Patient?",
    "text": "Is it the Patient?\n\nPatient over time\nOutcome colectomy\nlabs over time"
  },
  {
    "objectID": "slides/index.html#is-it-the-visitencounter",
    "href": "slides/index.html#is-it-the-visitencounter",
    "title": "Data Cleaning",
    "section": "Is it the Visit/Encounter?",
    "text": "Is it the Visit/Encounter?\n\nPatient over time\nOutcome colectomy\nlabs over time"
  },
  {
    "objectID": "slides/index.html#pivoting-longer-common",
    "href": "slides/index.html#pivoting-longer-common",
    "title": "Data Cleaning",
    "section": "Pivoting Longer (common)",
    "text": "Pivoting Longer (common)\n\nWe need to â€˜pivotâ€™ data from wide to tall on the regular\nThis â€œlengthensâ€ data, increasing the number of rows, and decreasing the number of columns\nWe will be looking at Visit Dates/Measures"
  },
  {
    "objectID": "slides/index.html#pivoting-wider",
    "href": "slides/index.html#pivoting-wider",
    "title": "Data Cleaning",
    "section": "Pivoting Wider",
    "text": "Pivoting Wider\n\nTall messy_uc DataCodeWider Result\n\n\n\nWide data is less common, but sometimes needed\nHere we will convert the tall version of our selected messy_uc data back to wide.\nThis is what the tall data look like\n\n\n\n# A tibble: 180 Ã— 4\n   pat_id treatment measure   score\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 001    upa       start_mes  3   \n 2 001    upa       end_mes    0   \n 3 001    upa       start_bss 75.5 \n 4 001    upa       end_bss    9.46\n 5 001    upa       start_emo 73.3 \n 6 001    upa       end_emo   32.0 \n 7 002    uste      start_mes  2   \n 8 002    uste      end_mes    1   \n 9 002    uste      start_bss 53.6 \n10 002    uste      end_bss   31.6 \n# â„¹ 170 more rows\n\n\n\n\n\ntall |&gt; \n  pivot_wider(\n    id_cols = c(pat_id, treatment), # Variables not pivoted\n    names_from = measure, # will become column names\n    values_from = score # will become values\n  )\n\n\n\n\n\n# A tibble: 30 Ã— 8\n   pat_id treatment start_mes end_mes start_bss end_bss start_emo end_emo\n   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 001    upa               3       0      75.5    9.46      73.3    32.0\n 2 002    uste              2       1      53.6   31.6       55.7    43.2\n 3 003    oza               1       1      25.5   25.5       36.6    36.9\n 4 004    upa               3       1      79.4   35.4       80.8    58.7\n 5 005    oza               3       2      79.5   57.5       72.5    58.9\n 6 006    uste              2       1      54.5   32.5       48.3    34.6\n 7 007    uste              3       2      79.0   57.0       72.7    63.3\n 8 008    oza               3       2      79.0   57.0       74.6    65.3\n 9 009    upa               2       0      53.1    9.06      54.2    28.0\n10 010    oza               3       1      77.4   33.4       74.4    46.8\n# â„¹ 20 more rows"
  },
  {
    "objectID": "slides/index.html#padding-unobserved-dates-weekends",
    "href": "slides/index.html#padding-unobserved-dates-weekends",
    "title": "Data Cleaning",
    "section": "Padding unobserved dates (weekends?)",
    "text": "Padding unobserved dates (weekends?)\n\n\n\nThe pad function allows you to fill in missing intervals.\nAs an example, my hospital only runs fecal calprotectin tests on weekdays.\nThis can lead to weird discontinuities in data over a weekend (Dec 3-4).\nNo observations on weekend days.\n\n\n\n\n# A tibble: 15 Ã— 3\n   pat_id date         fcp\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;\n 1 001    2022-12-01  1574\n 2 001    2022-12-02  1323\n 3 001    2022-12-05   673\n 4 001    2022-12-06   314\n 5 001    2022-12-07   168\n 6 002    2022-11-30  1393\n 7 002    2022-12-01  1014\n 8 002    2022-12-02   812\n 9 002    2022-12-05   247\n10 002    2022-12-06   118\n11 003    2022-12-02   987\n12 003    2022-12-05   438\n13 003    2022-12-06   312\n14 003    2022-12-05   194\n15 003    2022-12-06   101"
  },
  {
    "objectID": "slides/index.html#padding-unovserved-times",
    "href": "slides/index.html#padding-unovserved-times",
    "title": "Data Cleaning",
    "section": "Padding unovserved times",
    "text": "Padding unovserved times"
  },
  {
    "objectID": "slides/index.html#local-demographics-with-cdc-svi-data-one-to-one",
    "href": "slides/index.html#local-demographics-with-cdc-svi-data-one-to-one",
    "title": "Data Cleaning",
    "section": "Local Demographics with CDC SVI data (one to one)",
    "text": "Local Demographics with CDC SVI data (one to one)"
  },
  {
    "objectID": "slides/index.html#patient-demographics-with-lab-results-one-to-many",
    "href": "slides/index.html#patient-demographics-with-lab-results-one-to-many",
    "title": "Data Cleaning",
    "section": "Patient Demographics with Lab results (one to many)",
    "text": "Patient Demographics with Lab results (one to many)"
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This work is licensed under Creative Commons Zero v1.0 Universal.\n\n\n\n\n\nCrystal Lewis\n Shannon Pileggi\n\n\nPeter Higgins\n\nDuncan Garmonsway\n\n\n\n\n\n\n\n Install recent R release\nCurrent version 4.2.2\n Install RStudio\nI am on version 2022.07.1+554 \n Install packages\ninstall.packages(c(\"gtreg\", \"gtsummary\", \"tidyverse\", \"labelled\", \"usethis\"))\n Ensure you can knit Rmarkdown files\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n09:00 - 09:50\nSection 1\n\n\n09:50 - 10:00\nBreak\n\n\n10:00 - 10:50\nSection 2\n\n\n10:50 - 11:00\nBreak\n\n\n11:00 - 12:00\nSection 3\n\n\n\n\nPlease add any questions to the public Zoom chat. These may be answered in the moment or addressed at the end depending on context."
  },
  {
    "objectID": "exercises.html#exercise-1",
    "href": "exercises.html#exercise-1",
    "title": "Data Cleaning: Exercises",
    "section": "Exercise 1",
    "text": "Exercise 1\nview first six rows of mtcars\n\n\nShow the code solution\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "exercises.html#exercise-2",
    "href": "exercises.html#exercise-2",
    "title": "Data Cleaning: Exercises",
    "section": "Exercise 2",
    "text": "Exercise 2\nview first six rows of mtcars\n\n\nShow the code solution\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "exercises.html#exercise-3",
    "href": "exercises.html#exercise-3",
    "title": "Data Cleaning: Exercises",
    "section": "Exercise 3",
    "text": "Exercise 3\nview first six rows of mtcars\n\n\nShow the code solution\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "exercises.html#exercise-4",
    "href": "exercises.html#exercise-4",
    "title": "Data Cleaning: Exercises",
    "section": "Exercise 4",
    "text": "Exercise 4\nview first six rows of mtcars\n\n\nShow the code solution\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "exercises.html#exercise-5",
    "href": "exercises.html#exercise-5",
    "title": "Data Cleaning: Exercises",
    "section": "Exercise 5",
    "text": "Exercise 5\nview first six rows of mtcars\n\n\nShow the code solution\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "exercises.html#exercise-6",
    "href": "exercises.html#exercise-6",
    "title": "Data Cleaning: Exercises",
    "section": "Exercise 6",
    "text": "Exercise 6\nview first six rows of mtcars\n\n\nShow the code solution\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "exercises.html#exercise-7",
    "href": "exercises.html#exercise-7",
    "title": "Data Cleaning: Exercises",
    "section": "Exercise 7",
    "text": "Exercise 7\nview first six rows of mtcars\n\n\nShow the code solution\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "exercises.html#exercise-8",
    "href": "exercises.html#exercise-8",
    "title": "Data Cleaning: Exercises",
    "section": "Exercise 8",
    "text": "Exercise 8\nview first six rows of mtcars\n\n\nShow the code solution\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "slides/index.html#variable-names",
    "href": "slides/index.html#variable-names",
    "title": "Data Cleaning",
    "section": "Variable names",
    "text": "Variable names\nOriginal variable names in excel:\n\n\n\nVariable names import as shown, with modifications from readxl::read_excel() to ensure uniqueness:"
  },
  {
    "objectID": "slides/index.html#deciding-on-the-unit-of-observation",
    "href": "slides/index.html#deciding-on-the-unit-of-observation",
    "title": "Data Cleaning",
    "section": "Deciding on the Unit of Observation",
    "text": "Deciding on the Unit of Observation\n\nIn Prospective Data Collection, a patient arrives for a visit. All data collected are part of the same observation/visit.\n\nAlternatively, we can divide a visit into distinct observations, like blood pressure, a PHQ-9 depression questionnaire, and a hemoglobin measurement (all on the same date)\n\nIn retrospective data review, we can divide the observations up as we choose. For inpatient stays, we need to decide a priori on how to handle multiple observations of the same type (e.g., vitals q6h) in the same day.\n\nuse the 0800 observation each day?\nuse the daily average?\nuse the max values each day?"
  },
  {
    "objectID": "slides/index.html#reshaping-your-data-with-tidyr",
    "href": "slides/index.html#reshaping-your-data-with-tidyr",
    "title": "Data Cleaning",
    "section": "Reshaping your data with tidyr",
    "text": "Reshaping your data with tidyr\n\nWe often enter data by patient\nSpreadsheets encourage us to enter longitudinal data as long rows (per patient)\nWe end up with wide data\n\n[Image]"
  },
  {
    "objectID": "slides/index.html#joins-of-data-from-different-sources",
    "href": "slides/index.html#joins-of-data-from-different-sources",
    "title": "Data Cleaning",
    "section": "Joins of data from different sources",
    "text": "Joins of data from different sources\n\nWe often collect data from different sources that we later want to join together for analysis\n\nData from local Electronic Medical Record\nData from the CDC\nData from the US Census\n\nExternal data can illuminate our understanding of our local patient data"
  },
  {
    "objectID": "slides/index.html#variable-names-cleaner",
    "href": "slides/index.html#variable-names-cleaner",
    "title": "Data Cleaning",
    "section": "Variable names, cleaner",
    "text": "Variable names, cleaner\nVariable names as imported:\n\n\n\njanitor::clean_names() removes special characters and implements snake case by default:\n\ndf_clean &lt;- df_raw |&gt; \n  janitor::clean_names()"
  },
  {
    "objectID": "slides/index.html#recoding",
    "href": "slides/index.html#recoding",
    "title": "Data Cleaning",
    "section": "Recoding",
    "text": "Recoding\n\nProblemSolutionConfirm\n\n\n\ndf_clean |&gt; \n  count(ethnic)\n\n# A tibble: 5 Ã— 2\n  ethnic           n\n  &lt;chr&gt;        &lt;int&gt;\n1 Hispanic         1\n2 NOT hispanic     1\n3 hispamnic        1\n4 hispanic         3\n5 not hispanic    24\n\n\n\n\n\ndf_clean &lt;- df_raw |&gt;\n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    ethnic_clean = case_when(\n      ethnic %in%  c(\"hispanic\", \"Hispanic\", \"hispamnic\") ~ \"hispanic\",\n      ethnic %in%  c(\"NOT hispanic\", \"not hispanic\") ~ \"not hispanic\",\n    )\n  )\n\ndf_clean |&gt; \n  count(ethnic_clean)\n\n# A tibble: 2 Ã— 2\n  ethnic_clean     n\n  &lt;chr&gt;        &lt;int&gt;\n1 hispanic         5\n2 not hispanic    25\n\n\n\n\n\ndf_clean |&gt; \n  count(ethnic_clean, ethnic)\n\n# A tibble: 5 Ã— 3\n  ethnic_clean ethnic           n\n  &lt;chr&gt;        &lt;chr&gt;        &lt;int&gt;\n1 hispanic     Hispanic         1\n2 hispanic     hispamnic        1\n3 hispanic     hispanic         3\n4 not hispanic NOT hispanic     1\n5 not hispanic not hispanic    24"
  },
  {
    "objectID": "slides/index.html#tibble",
    "href": "slides/index.html#tibble",
    "title": "Data Cleaning",
    "section": "tibble",
    "text": "tibble\n\nmtcars |> tibble::as_tibble()\n\nA tibble: 32 Ã— 11\n mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n           1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 # â„¹ 22 more rows"
  },
  {
    "objectID": "slides/index.html#new-thing",
    "href": "slides/index.html#new-thing",
    "title": "Data Cleaning",
    "section": "new thing",
    "text": "new thing\n\nmtcars |&gt; tibble::as_tibble()\n\n# A tibble: 32 Ã— 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# â„¹ 22 more rows"
  },
  {
    "objectID": "slides/index.html#remove-empty-columns",
    "href": "slides/index.html#remove-empty-columns",
    "title": "Data Cleaning",
    "section": "Remove empty columns",
    "text": "Remove empty columns\n\nProblemSolutionConfirm\n\n\n\ndf_raw |&gt; \n  janitor::clean_names() |&gt; \n  select(pat_id, race:start_bp) |&gt; \n  slice(13:18)\n\n# A tibble: 6 Ã— 5\n  pat_id race             dob                 x7    start_bp\n  &lt;chr&gt;  &lt;chr&gt;            &lt;dttm&gt;              &lt;lgl&gt; &lt;chr&gt;   \n1 013    Caucasian        1948-02-27 00:00:00 NA    118/73  \n2 014    African-American 1966-04-22 00:00:00 NA    106/59  \n3 015    H/API            1978-08-11 00:00:00 NA    112/69  \n4 &lt;NA&gt;   &lt;NA&gt;             NA                  NA    &lt;NA&gt;    \n5 016    African-American 1998-10-28 00:00:00 NA    114/76  \n6 017    Caucasian        2001-01-09 00:00:00 NA    124/80  \n\n\n\n\n\ndf_raw |&gt; \n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  select(pat_id, race:start_bp) |&gt; \n  slice(13:18)\n\n# A tibble: 6 Ã— 4\n  pat_id race             dob                 start_bp\n  &lt;chr&gt;  &lt;chr&gt;            &lt;dttm&gt;              &lt;chr&gt;   \n1 013    Caucasian        1948-02-27 00:00:00 118/73  \n2 014    African-American 1966-04-22 00:00:00 106/59  \n3 015    H/API            1978-08-11 00:00:00 112/69  \n4 016    African-American 1998-10-28 00:00:00 114/76  \n5 017    Caucasian        2001-01-09 00:00:00 124/80  \n6 018    Caucasian        1994-03-07 00:00:00 120/68  \n\n\n\n\n\n\n\n#\ndf_raw |&gt;\n  janitor::clean_names() |&gt; \n  glimpse()\n\nRows: 31\nColumns: 38\n$ pat_id                           &lt;chr&gt; \"001\", \"002\", \"003\", \"004\", \"005\", \"0â€¦\n$ treatment                        &lt;chr&gt; \"upa\", \"uste\", \"oza\", \"upa\", \"oza\", \"â€¦\n$ start_date                       &lt;dttm&gt; 2021-01-12, 2021-01-19, 2021-02-03, â€¦\n$ ethnic                           &lt;chr&gt; \"hispanic\", \"not hispanic\", \"not hispâ€¦\n$ race                             &lt;chr&gt; \"Caucasian\", \"Caucasian\", \"African-Amâ€¦\n$ dob                              &lt;dttm&gt; 2005-01-07, 1937-04-13, 1946-06-06, â€¦\n$ x7                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ start_bp                         &lt;chr&gt; \"114/72\", \"132/86\", \"124/92\", \"144/83â€¦\n$ pre_post_wt_kg                   &lt;chr&gt; \"84/82\", \"77/77\", \"74/75\", \"66/65\", \"â€¦\n$ start_mes                        &lt;dbl&gt; 3, 2, 1, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3â€¦\n$ start_bss                        &lt;dbl&gt; 78.06060, 54.27825, 26.51270, 79.4443â€¦\n$ start_abd_score                  &lt;dbl&gt; 78.29165, 51.96852, 28.50220, 75.1931â€¦\n$ start_sys                        &lt;dbl&gt; 80.31764, 53.25294, 28.56385, 78.1731â€¦\n$ start_coping                     &lt;dbl&gt; 50.15692, 35.56621, 21.98904, 50.2882â€¦\n$ start_emo                        &lt;dbl&gt; 71.23417, 47.81382, 35.35624, 70.2985â€¦\n$ daily_life_impact_score_at_start &lt;dbl&gt; 85.93033, 59.09747, 30.75234, 87.4279â€¦\n$ start_wbc                        &lt;dbl&gt; 8.2, 10.1, 5.5, 4.7, 8.9, 9.3, 5.6, 9â€¦\n$ start_plt                        &lt;chr&gt; \"273\", \"414\", \"323\", \"389\", \"411\", \"4â€¦\n$ start_na                         &lt;dbl&gt; 137, 142, 140, 139, 144, 145, 142, 13â€¦\n$ start_k                          &lt;chr&gt; \"3.7\", \"4.0999999999999996\", \"4.3\", \"â€¦\n$ end_month                        &lt;dbl&gt; 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9â€¦\n$ end_day                          &lt;dbl&gt; 14, 21, 6, 22, 30, 4, 10, 15, 22, 26,â€¦\n$ end_year                         &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2â€¦\n$ end_mes                          &lt;dbl&gt; 0, 1, 1, 1, 2, 1, 2, 2, 0, 1, 0, 1, 1â€¦\n$ end_bss                          &lt;dbl&gt; 12.060596, 32.278245, 26.512702, 35.4â€¦\n$ end_abd                          &lt;dbl&gt; 18.29165, 31.96852, 28.50220, 35.1931â€¦\n$ end_sys                          &lt;dbl&gt; 8.317638, 29.252937, 28.563849, 30.17â€¦\n$ end_coping                       &lt;dbl&gt; 23.156925, 26.566214, 21.989042, 32.2â€¦\n$ end_emo                          &lt;chr&gt; \"30.275122309944972\", \"37.11825953633â€¦\n$ end_dl                           &lt;chr&gt; \"6.4360061166151254\", \"33.73708089887â€¦\n$ end_wbc                          &lt;dbl&gt; 9.211441, 10.415671, 3.000000, 4.9051â€¦\n$ end_plt                          &lt;dbl&gt; 228.61315, 346.13164, 305.44946, 341.â€¦\n$ end_na                           &lt;dbl&gt; 137.4247, 142.4211, 140.1604, 139.473â€¦\n$ end_k                            &lt;dbl&gt; 3.897774, 4.265951, 4.317939, 3.76078â€¦\n$ fake_street                      &lt;chr&gt; \"990Â MohammadÂ Mountain\", \"8512Â O'Connâ€¦\n$ fake_city                        &lt;chr&gt; \"NorthÂ Sigmundville\", \"PortÂ Halstad\",â€¦\n$ fake_state                       &lt;chr&gt; \"NewÂ Mexico\", \"Missouri\", \"SouthÂ Caroâ€¦\n$ fake_zip                         &lt;dbl&gt; 96074, 11264, 57246, 31457, 30711, 52â€¦\n\n\n\n\ndf_raw |&gt; \n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 37\n$ pat_id                           &lt;chr&gt; \"001\", \"002\", \"003\", \"004\", \"005\", \"0â€¦\n$ treatment                        &lt;chr&gt; \"upa\", \"uste\", \"oza\", \"upa\", \"oza\", \"â€¦\n$ start_date                       &lt;dttm&gt; 2021-01-12, 2021-01-19, 2021-02-03, â€¦\n$ ethnic                           &lt;chr&gt; \"hispanic\", \"not hispanic\", \"not hispâ€¦\n$ race                             &lt;chr&gt; \"Caucasian\", \"Caucasian\", \"African-Amâ€¦\n$ dob                              &lt;dttm&gt; 2005-01-07, 1937-04-13, 1946-06-06, â€¦\n$ start_bp                         &lt;chr&gt; \"114/72\", \"132/86\", \"124/92\", \"144/83â€¦\n$ pre_post_wt_kg                   &lt;chr&gt; \"84/82\", \"77/77\", \"74/75\", \"66/65\", \"â€¦\n$ start_mes                        &lt;dbl&gt; 3, 2, 1, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3â€¦\n$ start_bss                        &lt;dbl&gt; 78.06060, 54.27825, 26.51270, 79.4443â€¦\n$ start_abd_score                  &lt;dbl&gt; 78.29165, 51.96852, 28.50220, 75.1931â€¦\n$ start_sys                        &lt;dbl&gt; 80.31764, 53.25294, 28.56385, 78.1731â€¦\n$ start_coping                     &lt;dbl&gt; 50.15692, 35.56621, 21.98904, 50.2882â€¦\n$ start_emo                        &lt;dbl&gt; 71.23417, 47.81382, 35.35624, 70.2985â€¦\n$ daily_life_impact_score_at_start &lt;dbl&gt; 85.93033, 59.09747, 30.75234, 87.4279â€¦\n$ start_wbc                        &lt;dbl&gt; 8.2, 10.1, 5.5, 4.7, 8.9, 9.3, 5.6, 9â€¦\n$ start_plt                        &lt;chr&gt; \"273\", \"414\", \"323\", \"389\", \"411\", \"4â€¦\n$ start_na                         &lt;dbl&gt; 137, 142, 140, 139, 144, 145, 142, 13â€¦\n$ start_k                          &lt;chr&gt; \"3.7\", \"4.0999999999999996\", \"4.3\", \"â€¦\n$ end_month                        &lt;dbl&gt; 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9â€¦\n$ end_day                          &lt;dbl&gt; 14, 21, 6, 22, 30, 4, 10, 15, 22, 26,â€¦\n$ end_year                         &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2â€¦\n$ end_mes                          &lt;dbl&gt; 0, 1, 1, 1, 2, 1, 2, 2, 0, 1, 0, 1, 1â€¦\n$ end_bss                          &lt;dbl&gt; 12.060596, 32.278245, 26.512702, 35.4â€¦\n$ end_abd                          &lt;dbl&gt; 18.29165, 31.96852, 28.50220, 35.1931â€¦\n$ end_sys                          &lt;dbl&gt; 8.317638, 29.252937, 28.563849, 30.17â€¦\n$ end_coping                       &lt;dbl&gt; 23.156925, 26.566214, 21.989042, 32.2â€¦\n$ end_emo                          &lt;chr&gt; \"30.275122309944972\", \"37.11825953633â€¦\n$ end_dl                           &lt;chr&gt; \"6.4360061166151254\", \"33.73708089887â€¦\n$ end_wbc                          &lt;dbl&gt; 9.211441, 10.415671, 3.000000, 4.9051â€¦\n$ end_plt                          &lt;dbl&gt; 228.61315, 346.13164, 305.44946, 341.â€¦\n$ end_na                           &lt;dbl&gt; 137.4247, 142.4211, 140.1604, 139.473â€¦\n$ end_k                            &lt;dbl&gt; 3.897774, 4.265951, 4.317939, 3.76078â€¦\n$ fake_street                      &lt;chr&gt; \"990Â MohammadÂ Mountain\", \"8512Â O'Connâ€¦\n$ fake_city                        &lt;chr&gt; \"NorthÂ Sigmundville\", \"PortÂ Halstad\",â€¦\n$ fake_state                       &lt;chr&gt; \"NewÂ Mexico\", \"Missouri\", \"SouthÂ Caroâ€¦\n$ fake_zip                         &lt;dbl&gt; 96074, 11264, 57246, 31457, 30711, 52â€¦"
  },
  {
    "objectID": "slides/index.html#first-column",
    "href": "slides/index.html#first-column",
    "title": "Data Cleaning",
    "section": "First column",
    "text": "First column\nI would like to have text here\nSentence becomes longer, it should automatically stay in their column"
  },
  {
    "objectID": "slides/index.html#second-column",
    "href": "slides/index.html#second-column",
    "title": "Data Cleaning",
    "section": "Second column",
    "text": "Second column\nand here\nMore text"
  },
  {
    "objectID": "slides/index.html#replace-values-with-missing",
    "href": "slides/index.html#replace-values-with-missing",
    "title": "Data Cleaning",
    "section": "Replace values with missing",
    "text": "Replace values with missing\n\nProblemSolutionConfirm\n\n\n\n\n\ndf_clean |&gt; \n  count(end_na) \n\n# A tibble: 30 Ã— 2\n   end_na     n\n    &lt;dbl&gt; &lt;int&gt;\n 1   -99      1\n 2   133.     1\n 3   135.     1\n 4   135.     1\n 5   136.     1\n 6   137.     1\n 7   137.     1\n 8   138.     1\n 9   138.     1\n10   138.     1\n# â„¹ 20 more rows\n\n\n\n\ndf_clean |&gt; \n  ggplot(aes(x = end_na)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\ndf_clean &lt;- df_raw |&gt;\n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    ethnic_clean = case_when(\n      ethnic %in%  c(\"hispanic\", \"Hispanic\", \"hispamnic\") ~ \"hispanic\",\n      ethnic %in%  c(\"NOT hispanic\", \"not hispanic\") ~ \"not hispanic\",\n    ),\n    end_na_clean = na_if(end_na, -99)\n  ) \n\n\n\n\n\n\ndf_clean |&gt; \n  count(end_na, end_na_clean) \n\n# A tibble: 30 Ã— 3\n   end_na end_na_clean     n\n    &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;\n 1   -99           NA      1\n 2   133.         133.     1\n 3   135.         135.     1\n 4   135.         135.     1\n 5   136.         136.     1\n 6   137.         137.     1\n 7   137.         137.     1\n 8   138.         138.     1\n 9   138.         138.     1\n10   138.         138.     1\n# â„¹ 20 more rows\n\n\n\n\ndf_clean |&gt; \n  ggplot(aes(x = end_na_clean)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/index.html#remove-empty-columns-or-rows",
    "href": "slides/index.html#remove-empty-columns-or-rows",
    "title": "Data Cleaning",
    "section": "Remove empty columns or rows",
    "text": "Remove empty columns or rows\n\nProblemSolutionConfirm\n\n\n\ndf_clean |&gt; \n  select(pat_id, race:start_bp) |&gt; \n  slice(13:18)\n\n# A tibble: 6 Ã— 5\n  pat_id race             dob                 x7    start_bp\n  &lt;chr&gt;  &lt;chr&gt;            &lt;dttm&gt;              &lt;lgl&gt; &lt;chr&gt;   \n1 013    Caucasian        1948-02-27 00:00:00 NA    118/73  \n2 014    African-American 1966-04-22 00:00:00 NA    106/59  \n3 015    H/API            1978-08-11 00:00:00 NA    112/69  \n4 &lt;NA&gt;   &lt;NA&gt;             NA                  NA    &lt;NA&gt;    \n5 016    African-American 1998-10-28 00:00:00 NA    114/76  \n6 017    Caucasian        2001-01-09 00:00:00 NA    124/80  \n\n\n\n\n\ndf_clean &lt;- df_raw |&gt; \n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\"))\n\n\n\ndf_clean |&gt; \n  select(pat_id, race:start_bp) |&gt; \n  slice(13:18)\n\n# A tibble: 6 Ã— 4\n  pat_id race             dob                 start_bp\n  &lt;chr&gt;  &lt;chr&gt;            &lt;dttm&gt;              &lt;chr&gt;   \n1 013    Caucasian        1948-02-27 00:00:00 118/73  \n2 014    African-American 1966-04-22 00:00:00 106/59  \n3 015    H/API            1978-08-11 00:00:00 112/69  \n4 016    African-American 1998-10-28 00:00:00 114/76  \n5 017    Caucasian        2001-01-09 00:00:00 124/80  \n6 018    Caucasian        1994-03-07 00:00:00 120/68  \n\n\n\n\n\n\n\n#\ndf_raw |&gt;\n  janitor::clean_names() |&gt; \n  glimpse()\n\nRows: 31\nColumns: 38\n$ pat_id                           &lt;chr&gt; \"001\", \"002\", \"003\", \"004\", \"005\", \"0â€¦\n$ treatment                        &lt;chr&gt; \"upa\", \"uste\", \"oza\", \"upa\", \"oza\", \"â€¦\n$ start_date                       &lt;dbl&gt; 44208, 44215, 44230, 44245, 44255, 44â€¦\n$ ethnic                           &lt;chr&gt; \"hispanic\", \"not hispanic\", \"not hispâ€¦\n$ race                             &lt;chr&gt; \"Caucasian\", \"Caucasian\", \"African-Amâ€¦\n$ dob                              &lt;dttm&gt; 2005-01-07, 1937-04-13, 1946-06-06, â€¦\n$ x7                               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ start_bp                         &lt;chr&gt; \"114/72\", \"132/86\", \"124/92\", \"144/83â€¦\n$ pre_post_wt_kg                   &lt;chr&gt; \"84/82\", \"77/77\", \"74/75\", \"66/65\", \"â€¦\n$ start_mes                        &lt;dbl&gt; 3, 2, 1, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3â€¦\n$ start_bss                        &lt;dbl&gt; 75.45589, 53.62239, 25.52527, 79.3659â€¦\n$ start_abd_score                  &lt;dbl&gt; 80.59025, 53.64479, 26.15548, 77.9491â€¦\n$ start_sys                        &lt;dbl&gt; 81.45415, 52.60911, 27.55523, 79.8218â€¦\n$ start_coping                     &lt;dbl&gt; 50.41090, 29.56833, 15.20377, 51.2653â€¦\n$ start_emo                        &lt;dbl&gt; 73.32378, 55.72461, 36.56135, 80.7601â€¦\n$ daily_life_impact_score_at_start &lt;dbl&gt; 86.88945, 56.10371, 31.38942, 84.8523â€¦\n$ start_wbc                        &lt;dbl&gt; 8.2, 10.1, 5.5, 4.7, 8.9, 9.3, 5.6, 9â€¦\n$ start_plt                        &lt;chr&gt; \"273K/microL\", \"414K/microL\", \"323K/mâ€¦\n$ start_na                         &lt;chr&gt; \"137mmol/L\", \"142mmol/L\", \"140mmol/L\"â€¦\n$ start_k                          &lt;chr&gt; \"3.7\", \"4.0999999999999996\", \"4.3\", \"â€¦\n$ end_month                        &lt;dbl&gt; 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9â€¦\n$ end_day                          &lt;dbl&gt; 14, 21, 6, 22, 30, 4, 10, 15, 22, 26,â€¦\n$ end_year                         &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2â€¦\n$ end_mes                          &lt;dbl&gt; 0, 1, 1, 1, 2, 1, 2, 2, 0, 1, 0, 1, 1â€¦\n$ end_bss                          &lt;dbl&gt; 9.455894, 31.622388, 25.525267, 35.36â€¦\n$ end_abd                          &lt;dbl&gt; 20.590252, 33.644791, 26.155480, 37.9â€¦\n$ end_sys                          &lt;dbl&gt; 9.454153, 28.609114, 27.555232, 31.82â€¦\n$ end_coping                       &lt;dbl&gt; 23.41090, 20.56833, 15.20377, 33.2653â€¦\n$ end_emo                          &lt;chr&gt; \"31.980531232702354\", \"43.15496159247â€¦\n$ end_dl                           &lt;chr&gt; \"6.6605585857717573\", \"30.53917899804â€¦\n$ end_wbc                          &lt;dbl&gt; 8.562208, 11.135466, 3.000000, 5.6919â€¦\n$ end_plt                          &lt;dbl&gt; 201, 340, 256, 327, 432, 348, 181, 12â€¦\n$ end_na                           &lt;dbl&gt; 137.3278, 142.2140, 140.0831, 139.158â€¦\n$ end_k                            &lt;dbl&gt; 3.741212, 4.148464, 4.471147, 3.64134â€¦\n$ fake_street                      &lt;chr&gt; \"990Â MohammadÂ Mountain\", \"8512Â O'Connâ€¦\n$ fake_city                        &lt;chr&gt; \"NorthÂ Sigmundville\", \"PortÂ Halstad\",â€¦\n$ fake_state                       &lt;chr&gt; \"NewÂ Mexico\", \"Missouri\", \"SouthÂ Caroâ€¦\n$ fake_zip                         &lt;dbl&gt; 96074, 11264, 57246, 31457, 30711, 52â€¦\n\n\n\n\ndf_raw |&gt; \n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 37\n$ pat_id                           &lt;chr&gt; \"001\", \"002\", \"003\", \"004\", \"005\", \"0â€¦\n$ treatment                        &lt;chr&gt; \"upa\", \"uste\", \"oza\", \"upa\", \"oza\", \"â€¦\n$ start_date                       &lt;dbl&gt; 44208, 44215, 44230, 44245, 44255, 44â€¦\n$ ethnic                           &lt;chr&gt; \"hispanic\", \"not hispanic\", \"not hispâ€¦\n$ race                             &lt;chr&gt; \"Caucasian\", \"Caucasian\", \"African-Amâ€¦\n$ dob                              &lt;dttm&gt; 2005-01-07, 1937-04-13, 1946-06-06, â€¦\n$ start_bp                         &lt;chr&gt; \"114/72\", \"132/86\", \"124/92\", \"144/83â€¦\n$ pre_post_wt_kg                   &lt;chr&gt; \"84/82\", \"77/77\", \"74/75\", \"66/65\", \"â€¦\n$ start_mes                        &lt;dbl&gt; 3, 2, 1, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3â€¦\n$ start_bss                        &lt;dbl&gt; 75.45589, 53.62239, 25.52527, 79.3659â€¦\n$ start_abd_score                  &lt;dbl&gt; 80.59025, 53.64479, 26.15548, 77.9491â€¦\n$ start_sys                        &lt;dbl&gt; 81.45415, 52.60911, 27.55523, 79.8218â€¦\n$ start_coping                     &lt;dbl&gt; 50.41090, 29.56833, 15.20377, 51.2653â€¦\n$ start_emo                        &lt;dbl&gt; 73.32378, 55.72461, 36.56135, 80.7601â€¦\n$ daily_life_impact_score_at_start &lt;dbl&gt; 86.88945, 56.10371, 31.38942, 84.8523â€¦\n$ start_wbc                        &lt;dbl&gt; 8.2, 10.1, 5.5, 4.7, 8.9, 9.3, 5.6, 9â€¦\n$ start_plt                        &lt;chr&gt; \"273K/microL\", \"414K/microL\", \"323K/mâ€¦\n$ start_na                         &lt;chr&gt; \"137mmol/L\", \"142mmol/L\", \"140mmol/L\"â€¦\n$ start_k                          &lt;chr&gt; \"3.7\", \"4.0999999999999996\", \"4.3\", \"â€¦\n$ end_month                        &lt;dbl&gt; 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9â€¦\n$ end_day                          &lt;dbl&gt; 14, 21, 6, 22, 30, 4, 10, 15, 22, 26,â€¦\n$ end_year                         &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2â€¦\n$ end_mes                          &lt;dbl&gt; 0, 1, 1, 1, 2, 1, 2, 2, 0, 1, 0, 1, 1â€¦\n$ end_bss                          &lt;dbl&gt; 9.455894, 31.622388, 25.525267, 35.36â€¦\n$ end_abd                          &lt;dbl&gt; 20.590252, 33.644791, 26.155480, 37.9â€¦\n$ end_sys                          &lt;dbl&gt; 9.454153, 28.609114, 27.555232, 31.82â€¦\n$ end_coping                       &lt;dbl&gt; 23.41090, 20.56833, 15.20377, 33.2653â€¦\n$ end_emo                          &lt;chr&gt; \"31.980531232702354\", \"43.15496159247â€¦\n$ end_dl                           &lt;chr&gt; \"6.6605585857717573\", \"30.53917899804â€¦\n$ end_wbc                          &lt;dbl&gt; 8.562208, 11.135466, 3.000000, 5.6919â€¦\n$ end_plt                          &lt;dbl&gt; 201, 340, 256, 327, 432, 348, 181, 12â€¦\n$ end_na                           &lt;dbl&gt; 137.3278, 142.2140, 140.0831, 139.158â€¦\n$ end_k                            &lt;dbl&gt; 3.741212, 4.148464, 4.471147, 3.64134â€¦\n$ fake_street                      &lt;chr&gt; \"990Â MohammadÂ Mountain\", \"8512Â O'Connâ€¦\n$ fake_city                        &lt;chr&gt; \"NorthÂ Sigmundville\", \"PortÂ Halstad\",â€¦\n$ fake_state                       &lt;chr&gt; \"NewÂ Mexico\", \"Missouri\", \"SouthÂ Caroâ€¦\n$ fake_zip                         &lt;dbl&gt; 96074, 11264, 57246, 31457, 30711, 52â€¦"
  },
  {
    "objectID": "slides/index.html#incorrect-variable-type",
    "href": "slides/index.html#incorrect-variable-type",
    "title": "Data Cleaning",
    "section": "Incorrect variable type",
    "text": "Incorrect variable type\n\nProblemSolutionConfirm\n\n\n\n\n\ndf_raw |&gt; \n  select(end_emo) |&gt; \n  glimpse()\n\nRows: 31\nColumns: 1\n$ end_emo &lt;chr&gt; \"31.980531232702354\", \"43.154961592472482\", \"36.88112394645107â€¦\n\n\n\n\n\nmean(df_raw[[\"end_emo\"]], na.rm = TRUE)\n\nWarning in mean.default(df_raw[[\"end_emo\"]], na.rm = TRUE): argument is not\nnumeric or logical: returning NA\n\n\n[1] NA\n\n\n\n\n\ndf_raw[[\"end_emo\"]]\n\n [1] \"31.980531232702354\" \"43.154961592472482\" \"36.881123946451076\"\n [4] \"58.670309483569042\" \"58.863864661125419\" \"34.593046073414015\"\n [7] \"63.285059520403976\" \"65.313599031979081\" \"27.976656095216335\"\n[10] \"46.802992507671597\" \"33.122563864954309\" \"49.240520034972612\"\n[13] \"47.050604139781761\" \"54.487640962873577\" \"not done\"          \n[16] NA                   \"49.084529664712441\" \"27.370965295845295\"\n[19] \"60.432712720552317\" \"26.162987564588903\" \"48.329539382030802\"\n[22] \"40.735196376550661\" \"27.000188739872502\" \"57.019771433515629\"\n[25] \"39.783229029414606\" \"52.110256961065794\" \"37.098188331548307\"\n[28] \"39.264033750725694\" \"70.34798440037369\"  \"29.839211956263874\"\n[31] \"42.853436653960713\"\n\n\n\n\n\n\n\ndf_clean &lt;- df_raw |&gt;\n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    ethnic_clean = case_when(\n      ethnic %in%  c(\"hispanic\", \"Hispanic\", \"hispamnic\") ~ \"hispanic\",\n      ethnic %in%  c(\"NOT hispanic\", \"not hispanic\") ~ \"not hispanic\",\n    ),\n    end_na_clean = na_if(end_na, -99),\n    end_emo_clean = na_if(end_emo, \"not done\") |&gt; as.numeric()\n  ) \n\n\n\n\n\n\ndf_clean |&gt; \n  select(end_emo_clean) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 1\n$ end_emo_clean &lt;dbl&gt; 31.98053, 43.15496, 36.88112, 58.67031, 58.86386, 34.593â€¦\n\n\n\n\n\nmean(df_clean[[\"end_emo_clean\"]], na.rm = TRUE)\n\n[1] 44.78813\n\n\n\n\n\ndf_clean |&gt; \n  count(end_emo_clean, end_emo)\n\n# A tibble: 30 Ã— 3\n   end_emo_clean end_emo                n\n           &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;\n 1          26.2 26.162987564588903     1\n 2          27.0 27.000188739872502     1\n 3          27.4 27.370965295845295     1\n 4          28.0 27.976656095216335     1\n 5          29.8 29.839211956263874     1\n 6          32.0 31.980531232702354     1\n 7          33.1 33.122563864954309     1\n 8          34.6 34.593046073414015     1\n 9          36.9 36.881123946451076     1\n10          37.1 37.098188331548307     1\n# â„¹ 20 more rows"
  },
  {
    "objectID": "slides/index.html#character-variable-should-be-a-factor",
    "href": "slides/index.html#character-variable-should-be-a-factor",
    "title": "Data Cleaning",
    "section": "Character variable should be a factor",
    "text": "Character variable should be a factor\n\nProblemSolutionConfirm\n\n\n\n\n\ndf_clean |&gt; \n  count(treatment)\n\n# A tibble: 3 Ã— 2\n  treatment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 oza          10\n2 upa          10\n3 uste         10\n\n\n\n\n\ndf_clean |&gt; \n  count(ethnic_clean)\n\n# A tibble: 2 Ã— 2\n  ethnic_clean     n\n  &lt;chr&gt;        &lt;int&gt;\n1 hispanic         5\n2 not hispanic    25\n\n\n\n\n\ndf_clean |&gt; \n  select(treatment, ethnic_clean) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 2\n$ treatment    &lt;chr&gt; \"upa\", \"uste\", \"oza\", \"upa\", \"oza\", \"uste\", \"uste\", \"oza\"â€¦\n$ ethnic_clean &lt;chr&gt; \"hispanic\", \"not hispanic\", \"not hispanic\", \"not hispanicâ€¦\n\n\n\n\ndf_clean |&gt; \n  select(treatment, ethnic_clean) |&gt; \n  tbl_summary(by = treatment)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      oza, N = 101\n      upa, N = 101\n      uste, N = 101\n    \n  \n  \n    ethnic_clean\n\n\n\n    Â Â Â Â hispanic\n4 (40%)\n1 (10%)\n0 (0%)\n    Â Â Â Â not hispanic\n6 (60%)\n9 (90%)\n10 (100%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\n\n\n\n\n\ndf_clean &lt;- df_raw |&gt;\n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    ethnic_clean = case_when(\n      ethnic %in%  c(\"hispanic\", \"Hispanic\", \"hispamnic\") ~ \"hispanic\",\n      ethnic %in%  c(\"NOT hispanic\", \"not hispanic\") ~ \"not hispanic\",\n    ) |&gt; fct_infreq(),\n    end_na_clean = na_if(end_na, -99),\n    end_emo_clean = na_if(end_emo, \"not done\") |&gt; as.numeric(),\n    start_na_clean = parse_number(start_na),\n    treatment = fct_relevel(treatment, \"upa\", \"uste\", \"oza\")\n  ) \n\n\nSee the forcats package for other factor handling solutions.\n\n\n\n\n\ndf_clean |&gt; \n  count(treatment)\n\n# A tibble: 3 Ã— 2\n  treatment     n\n  &lt;fct&gt;     &lt;int&gt;\n1 upa          10\n2 uste         10\n3 oza          10\n\n\n\n\n\ndf_clean |&gt; \n  count(ethnic_clean)\n\n# A tibble: 2 Ã— 2\n  ethnic_clean     n\n  &lt;fct&gt;        &lt;int&gt;\n1 not hispanic    25\n2 hispanic         5\n\n\n\n\n\ndf_clean |&gt; \n  select(treatment, ethnic_clean) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 2\n$ treatment    &lt;fct&gt; upa, uste, oza, upa, oza, uste, uste, oza, upa, oza, upa,â€¦\n$ ethnic_clean &lt;fct&gt; hispanic, not hispanic, not hispanic, not hispanic, not hâ€¦\n\n\n\n\ndf_clean |&gt; \n  select(treatment, ethnic_clean) |&gt; \n  tbl_summary(by = treatment)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      upa, N = 101\n      uste, N = 101\n      oza, N = 101\n    \n  \n  \n    ethnic_clean\n\n\n\n    Â Â Â Â not hispanic\n9 (90%)\n10 (100%)\n6 (60%)\n    Â Â Â Â hispanic\n1 (10%)\n0 (0%)\n4 (40%)\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "slides/index.html#separating-values",
    "href": "slides/index.html#separating-values",
    "title": "Data Cleaning",
    "section": "Separating values",
    "text": "Separating values\n\nProblemSolutionConfirm\n\n\n\ndf_clean |&gt; \n  select(start_bp) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 1\n$ start_bp &lt;chr&gt; \"114/72\", \"132/86\", \"124/92\", \"144/83\", \"122/78\", \"121/80\", \"â€¦\n\n\n\n\n\nmean(df_clean[[\"start_bp\"]], na.rm = TRUE)\n\n[1] NA\n\n\n\n\n\n\ndf_clean[[\"start_bp\"]]\n\n [1] \"114/72\" \"132/86\" \"124/92\" \"144/83\" \"122/78\" \"121/80\" \"133/74\" \"116/73\"\n [9] \"118/66\" \"122/78\" \"126/82\" \"114/68\" \"118/73\" \"106/59\" \"112/69\" \"114/76\"\n[17] \"124/80\" \"120/68\" \"119/77\" \"116/74\" \"121/80\" \"112/58\" \"117/67\" \"118/73\"\n[25] \"116/74\" \"126/84\" \"144/96\" \"120/84\" \"115/75\" \"142/92\"\n\n\n\n\n\n\ndf_clean &lt;- df_raw |&gt;\n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    ethnic_clean = case_when(\n      ethnic %in%  c(\"hispanic\", \"Hispanic\", \"hispamnic\") ~ \"hispanic\",\n      ethnic %in%  c(\"NOT hispanic\", \"not hispanic\") ~ \"not hispanic\",\n    ) |&gt; fct_infreq(),\n    end_na_clean = na_if(end_na, -99),\n    end_emo_clean = na_if(end_emo, \"not done\") |&gt; as.numeric(),\n    start_na_clean = parse_number(start_na),\n    treatment = fct_relevel(treatment, \"upa\", \"uste\", \"oza\")\n    ) |&gt;  \n  separate_wider_delim(start_bp, delim =\"/\", names = c(\"bp_systolic\", \"bp_diastolic\"), cols_remove = FALSE) |&gt; \n  mutate(across(c(bp_systolic, bp_diastolic), as.numeric)) \n\n\n\n\n\n\ndf_clean |&gt; \n  select(start_bp, bp_systolic, bp_diastolic) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 3\n$ start_bp     &lt;chr&gt; \"114/72\", \"132/86\", \"124/92\", \"144/83\", \"122/78\", \"121/80â€¦\n$ bp_systolic  &lt;dbl&gt; 114, 132, 124, 144, 122, 121, 133, 116, 118, 122, 126, 11â€¦\n$ bp_diastolic &lt;dbl&gt; 72, 86, 92, 83, 78, 80, 74, 73, 66, 78, 82, 68, 73, 59, 6â€¦\n\n\n\n\n\nmean(df_clean[[\"bp_systolic\"]], na.rm = TRUE)\n\n[1] 121.5333\n\nmean(df_clean[[\"bp_diastolic\"]], na.rm = TRUE)\n\n[1] 76.36667\n\n\n\n\n\n\ndf_clean[[\"bp_systolic\"]]\n\n [1] 114 132 124 144 122 121 133 116 118 122 126 114 118 106 112 114 124 120 119\n[20] 116 121 112 117 118 116 126 144 120 115 142\n\ndf_clean[[\"bp_diastolic\"]]\n\n [1] 72 86 92 83 78 80 74 73 66 78 82 68 73 59 69 76 80 68 77 74 80 58 67 73 74\n[26] 84 96 84 75 92"
  },
  {
    "objectID": "slides/index.html#assigning-labels",
    "href": "slides/index.html#assigning-labels",
    "title": "Data Cleaning",
    "section": "Assigning labels",
    "text": "Assigning labels\n\nProblemSolution 1Solution 2Solution 3Confirm 1Confirm 2\n\n\nWhat does anything mean?\n\nview(df_clean)\n\n\n\n\n\n\n# first import codebook\ndf_codebook &lt;- read_excel(\n  path = here(\"data\", \"messy_uc.xlsx\"),\n  sheet = \"Codebook\"\n)\ndf_codebook\n\n# A tibble: 33 Ã— 5\n   Variable       Details                                      Units Range ...5 \n   &lt;chr&gt;          &lt;chr&gt;                                        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 pat_id         Patient Identifier                           digiâ€¦ 001-â€¦ &lt;NA&gt; \n 2 treatment      Treatment for UC                             &lt;NA&gt;  upadâ€¦ &lt;NA&gt; \n 3 start_date     Date of start of treatment                   digiâ€¦ YYYYâ€¦ &lt;NA&gt; \n 4 ethnic         Ethnicity - hispanic or not hispanic         &lt;NA&gt;  hispâ€¦ &lt;NA&gt; \n 5 race           Race - one of 7 choices                      &lt;NA&gt;  caucâ€¦ &lt;NA&gt; \n 6 dob            date of birth                                digiâ€¦ YYYYâ€¦ &lt;NA&gt; \n 7 start_bp       blood pressure at start - systolic/diastolic mm Hg systâ€¦ &lt;NA&gt; \n 8 pre/post_wt_kg weight in kilograms at start/end             kiloâ€¦ 48-1â€¦ &lt;NA&gt; \n 9 start_mes      Mayo endoscopic Score at start of treatment  poinâ€¦ 0-3   &lt;NA&gt; \n10 start_bss      Bowel symptom score at start                 poinâ€¦ 0-100 QOL â€¦\n# â„¹ 23 more rows\n\n\n\n\n\n# second create a named vector of variable names and variable labels\nvec_variables &lt;- df_codebook |&gt; \n  select(Variable, Details) |&gt; \n  deframe()\n\nvec_variables\n\n                                        pat_id \n                          \"Patient Identifier\" \n                                     treatment \n                            \"Treatment for UC\" \n                                    start_date \n                  \"Date of start of treatment\" \n                                        ethnic \n        \"Ethnicity - hispanic or not hispanic\" \n                                          race \n                     \"Race - one of 7 choices\" \n                                           dob \n                               \"date of birth\" \n                                      start_bp \n\"blood pressure at start - systolic/diastolic\" \n                                pre/post_wt_kg \n            \"weight in kilograms at start/end\" \n                                     start_mes \n \"Mayo endoscopic Score at start of treatment\" \n                                     start_bss \n                \"Bowel symptom score at start\" \n                                     start_abd \n            \"Abdominal symptom score at start\" \n                                     start_sys \n             \"Systemic symptom score at start\" \n                                  start_coping \n                       \"Coping score at start\" \n                                     start_emo \n            \"Emotional symptom score at start\" \n                                      start_dl \n       \"Impact on Daily living score at start\" \n                                     start_wbc \n    \"White blood cell count in blood at start\" \n                                     start_plt \n            \"Platelet count in blood at start\" \n                                      start_na \n              \"Sodium level in serum at start\" \n                                       start_k \n           \"Potassium level in serum at start\" \n                                     end_month \n                          \"Month of end visit\" \n                                       end_day \n                            \"Day of end visit\" \n                                      end_year \n                           \"Year of end visit\" \n                                       end_mes \n   \"Mayo endoscopic Score at end of treatment\" \n                                       end_bss \n                  \"Bowel symptom score at end\" \n                                       end_abd \n              \"Abdominal symptom score at end\" \n                                       end_sys \n               \"Systemic symptom score at end\" \n                                    end_coping \n                         \"Coping score at end\" \n                                       end_emo \n              \"Emotional symptom score at end\" \n                                        end_dl \n         \"Impact on Daily living score at end\" \n                                       end_wbc \n      \"White blood cell count in blood at end\" \n                                       end_plt \n              \"Platelet count in blood at end\" \n                                        end_na \n                \"Sodium level in serum at end\" \n                                         end_k \n             \"Potassium level in serum at end\" \n\n\n\n\n\n# assign labels to the data set\ndf_clean &lt;- df_raw |&gt;\n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    ethnic_clean = case_when(\n      ethnic %in%  c(\"hispanic\", \"Hispanic\", \"hispamnic\") ~ \"hispanic\",\n      ethnic %in%  c(\"NOT hispanic\", \"not hispanic\") ~ \"not hispanic\",\n    ) |&gt; fct_infreq(),\n    end_na_clean = na_if(end_na, -99),\n    end_emo_clean = na_if(end_emo, \"not done\") |&gt; as.numeric(),\n    start_na_clean = parse_number(start_na),\n    treatment = fct_relevel(treatment, \"upa\", \"uste\", \"oza\")\n    ) |&gt;  \n  separate_wider_delim(start_bp, delim =\"/\", names = c(\"bp_systolic\", \"bp_diastolic\"), cols_remove = FALSE) |&gt; \n  mutate(across(c(bp_systolic, bp_diastolic), as.numeric)) |&gt; \n  # assign labels to all variables from the codebook\n  labelled::set_variable_labels(!!!vec_variables, .strict = FALSE) |&gt; \n  # assign labels to new derived variables that did not exist in the code book\n  labelled::set_variable_labels(\n    ethnic_clean = \"Ethnicity\",\n    end_na_clean = \"Sodium level in serum at end\",\n    end_emo_clean = \"Emotional symptom score at end\",\n    bp_systolic = \"Systolic blood pressure\",\n    bp_diastolic = \"Diastolic blood pressure\"\n  )\n\n\n\n\n# view entire data set\ndf_clean |&gt; view()\n\n\n\n\n\n\n# view structure of data frame\ndf_clean |&gt; str()\n\ntibble [30 Ã— 43] (S3: tbl_df/tbl/data.frame)\n $ pat_id                          : chr [1:30] \"001\" \"002\" \"003\" \"004\" ...\n  ..- attr(*, \"label\")= chr \"Patient Identifier\"\n $ treatment                       : Factor w/ 3 levels \"upa\",\"uste\",\"oza\": 1 2 3 1 3 2 2 3 1 3 ...\n  ..- attr(*, \"label\")= chr \"Treatment for UC\"\n $ start_date                      : num [1:30] 44208 44215 44230 44245 44255 ...\n  ..- attr(*, \"label\")= chr \"Date of start of treatment\"\n $ ethnic                          : chr [1:30] \"hispanic\" \"not hispanic\" \"not hispanic\" \"not hispanic\" ...\n  ..- attr(*, \"label\")= chr \"Ethnicity - hispanic or not hispanic\"\n $ race                            : chr [1:30] \"Caucasian\" \"Caucasian\" \"African-American\" \"Caucasian\" ...\n  ..- attr(*, \"label\")= chr \"Race - one of 7 choices\"\n $ dob                             : POSIXct[1:30], format: \"2005-01-07\" \"1937-04-13\" ...\n $ bp_systolic                     : num [1:30] 114 132 124 144 122 121 133 116 118 122 ...\n  ..- attr(*, \"label\")= chr \"Systolic blood pressure\"\n $ bp_diastolic                    : num [1:30] 72 86 92 83 78 80 74 73 66 78 ...\n  ..- attr(*, \"label\")= chr \"Diastolic blood pressure\"\n $ start_bp                        : chr [1:30] \"114/72\" \"132/86\" \"124/92\" \"144/83\" ...\n  ..- attr(*, \"label\")= chr \"blood pressure at start - systolic/diastolic\"\n $ pre_post_wt_kg                  : chr [1:30] \"84/82\" \"77/77\" \"74/75\" \"66/65\" ...\n $ start_mes                       : num [1:30] 3 2 1 3 3 2 3 3 2 3 ...\n  ..- attr(*, \"label\")= chr \"Mayo endoscopic Score at start of treatment\"\n $ start_bss                       : num [1:30] 75.5 53.6 25.5 79.4 79.5 ...\n  ..- attr(*, \"label\")= chr \"Bowel symptom score at start\"\n $ start_abd_score                 : num [1:30] 80.6 53.6 26.2 77.9 78.4 ...\n $ start_sys                       : num [1:30] 81.5 52.6 27.6 79.8 79.2 ...\n  ..- attr(*, \"label\")= chr \"Systemic symptom score at start\"\n $ start_coping                    : num [1:30] 50.4 29.6 15.2 51.3 43.1 ...\n  ..- attr(*, \"label\")= chr \"Coping score at start\"\n $ start_emo                       : num [1:30] 73.3 55.7 36.6 80.8 72.5 ...\n  ..- attr(*, \"label\")= chr \"Emotional symptom score at start\"\n $ daily_life_impact_score_at_start: num [1:30] 86.9 56.1 31.4 84.9 87.3 ...\n $ start_wbc                       : num [1:30] 8.2 10.1 5.5 4.7 8.9 9.3 5.6 9.7 8.3 7.6 ...\n  ..- attr(*, \"label\")= chr \"White blood cell count in blood at start\"\n $ start_plt                       : chr [1:30] \"273K/microL\" \"414K/microL\" \"323K/microL\" \"389K/microL\" ...\n  ..- attr(*, \"label\")= chr \"Platelet count in blood at start\"\n $ start_na                        : chr [1:30] \"137mmol/L\" \"142mmol/L\" \"140mmol/L\" \"139mmol/L\" ...\n  ..- attr(*, \"label\")= chr \"Sodium level in serum at start\"\n $ start_k                         : chr [1:30] \"3.7\" \"4.0999999999999996\" \"4.3\" \"3.5\" ...\n  ..- attr(*, \"label\")= chr \"Potassium level in serum at start\"\n $ end_month                       : num [1:30] 6 6 7 7 7 8 8 8 8 8 ...\n  ..- attr(*, \"label\")= chr \"Month of end visit\"\n $ end_day                         : num [1:30] 14 21 6 22 30 4 10 15 22 26 ...\n  ..- attr(*, \"label\")= chr \"Day of end visit\"\n $ end_year                        : num [1:30] 2021 2021 2021 2021 2021 ...\n  ..- attr(*, \"label\")= chr \"Year of end visit\"\n $ end_mes                         : num [1:30] 0 1 1 1 2 1 2 2 0 1 ...\n  ..- attr(*, \"label\")= chr \"Mayo endoscopic Score at end of treatment\"\n $ end_bss                         : num [1:30] 9.46 31.62 25.53 35.37 57.53 ...\n  ..- attr(*, \"label\")= chr \"Bowel symptom score at end\"\n $ end_abd                         : num [1:30] 20.6 33.6 26.2 37.9 58.4 ...\n  ..- attr(*, \"label\")= chr \"Abdominal symptom score at end\"\n $ end_sys                         : num [1:30] 9.45 28.61 27.56 31.82 55.19 ...\n  ..- attr(*, \"label\")= chr \"Systemic symptom score at end\"\n $ end_coping                      : num [1:30] 23.4 20.6 15.2 33.3 34.1 ...\n  ..- attr(*, \"label\")= chr \"Coping score at end\"\n $ end_emo                         : chr [1:30] \"31.980531232702354\" \"43.154961592472482\" \"36.881123946451076\" \"58.670309483569042\" ...\n  ..- attr(*, \"label\")= chr \"Emotional symptom score at end\"\n $ end_dl                          : chr [1:30] \"6.6605585857717573\" \"30.539178998046562\" \"32.798667784920319\" \"33.781373664703942\" ...\n  ..- attr(*, \"label\")= chr \"Impact on Daily living score at end\"\n $ end_wbc                         : num [1:30] 8.56 11.14 3 5.69 4.8 ...\n  ..- attr(*, \"label\")= chr \"White blood cell count in blood at end\"\n $ end_plt                         : num [1:30] 201 340 256 327 432 348 181 128 135 238 ...\n  ..- attr(*, \"label\")= chr \"Platelet count in blood at end\"\n $ end_na                          : num [1:30] 137 142 140 139 144 ...\n  ..- attr(*, \"label\")= chr \"Sodium level in serum at end\"\n $ end_k                           : num [1:30] 3.74 4.15 4.47 3.64 4.21 ...\n  ..- attr(*, \"label\")= chr \"Potassium level in serum at end\"\n $ fake_street                     : chr [1:30] \"990Â MohammadÂ Mountain\" \"8512Â O'ConnellÂ Valley\" \"777Â LednerÂ Mall\" \"690Â AndresÂ Village\" ...\n $ fake_city                       : chr [1:30] \"NorthÂ Sigmundville\" \"PortÂ Halstad\" \"Croninton\" \"SouthÂ Isaac\" ...\n $ fake_state                      : chr [1:30] \"NewÂ Mexico\" \"Missouri\" \"SouthÂ Carolina\" \"Montana\" ...\n $ fake_zip                        : num [1:30] 96074 11264 57246 31457 30711 ...\n $ ethnic_clean                    : Factor w/ 2 levels \"not hispanic\",..: 2 1 1 1 1 1 1 2 1 2 ...\n  ..- attr(*, \"label\")= chr \"Ethnicity\"\n $ end_na_clean                    : num [1:30] 137 142 140 139 144 ...\n  ..- attr(*, \"label\")= chr \"Sodium level in serum at end\"\n $ end_emo_clean                   : num [1:30] 32 43.2 36.9 58.7 58.9 ...\n  ..- attr(*, \"label\")= chr \"Emotional symptom score at end\"\n $ start_na_clean                  : num [1:30] 137 142 140 139 144 145 142 138 140 137 ..."
  },
  {
    "objectID": "slides/index.html#extracting-numbers-from-text",
    "href": "slides/index.html#extracting-numbers-from-text",
    "title": "Data Cleaning",
    "section": "Extracting numbers from text",
    "text": "Extracting numbers from text\n\nProblemSolutionConfirm\n\n\n\n\n\ndf_raw |&gt; \n  select(start_na) |&gt; \n  glimpse()\n\nRows: 31\nColumns: 1\n$ start_na &lt;chr&gt; \"137mmol/L\", \"142mmol/L\", \"140mmol/L\", \"139mmol/L\", \"144mmol/â€¦\n\n\n\n\n\nmean(df_raw[[\"start_na\"]], na.rm = TRUE)\n\nWarning in mean.default(df_raw[[\"start_na\"]], na.rm = TRUE): argument is not\nnumeric or logical: returning NA\n\n\n[1] NA\n\n\n\n\n\ndf_raw[[\"start_na\"]]\n\n [1] \"137mmol/L\" \"142mmol/L\" \"140mmol/L\" \"139mmol/L\" \"144mmol/L\" \"145mmol/L\"\n [7] \"142mmol/L\" \"138mmol/L\" \"140mmol/L\" \"137mmol/L\" \"143mmol/L\" \"136mmol/L\"\n[13] \"135mmol/L\" \"141mmol/L\" \"133mmol/L\" NA          \"135mmol/L\" \"143mmol/L\"\n[19] \"136mmol/L\" \"144mmol/L\" \"145mmol/L\" \"140mmol/L\" \"141mmol/L\" \"142mmol/L\"\n[25] \"138mmol/L\" \"139mmol/L\" \"142mmol/L\" \"144mmol/L\" \"139mmol/L\" \"138mmol/L\"\n[31] \"140mmol/L\"\n\n\n\n\n\n\n\ndf_clean &lt;- df_raw |&gt;\n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    ethnic_clean = case_when(\n      ethnic %in%  c(\"hispanic\", \"Hispanic\", \"hispamnic\") ~ \"hispanic\",\n      ethnic %in%  c(\"NOT hispanic\", \"not hispanic\") ~ \"not hispanic\",\n    ),\n    end_na_clean = na_if(end_na, -99),\n    end_emo_clean = na_if(end_emo, \"not done\") |&gt; as.numeric(),\n    start_na_clean = parse_number(start_na)\n  ) \n\n\n\n\n\n\ndf_clean |&gt; \n  select(start_na_clean) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 1\n$ start_na_clean &lt;dbl&gt; 137, 142, 140, 139, 144, 145, 142, 138, 140, 137, 143, â€¦\n\n\n\n\n\nmean(df_clean[[\"start_na_clean\"]], na.rm = TRUE)\n\n[1] 139.9333\n\n\n\n\n\ndf_clean |&gt; \n  count(start_na_clean, start_na)\n\n# A tibble: 12 Ã— 3\n   start_na_clean start_na      n\n            &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt;\n 1            133 133mmol/L     1\n 2            135 135mmol/L     2\n 3            136 136mmol/L     2\n 4            137 137mmol/L     2\n 5            138 138mmol/L     3\n 6            139 139mmol/L     3\n 7            140 140mmol/L     4\n 8            141 141mmol/L     2\n 9            142 142mmol/L     4\n10            143 143mmol/L     2\n11            144 144mmol/L     3\n12            145 145mmol/L     2"
  },
  {
    "objectID": "slides/index.html#correcting-dates",
    "href": "slides/index.html#correcting-dates",
    "title": "Data Cleaning",
    "section": "Correcting dates",
    "text": "Correcting dates\n\nProblemSolutionConfirm\n\n\n\ndf_raw |&gt; \n  select(start_date) |&gt; \n  glimpse()\n\nRows: 31\nColumns: 1\n$ start_date &lt;dbl&gt; 44208, 44215, 44230, 44245, 44255, 44259, 44264, 44999, 442â€¦\n\n\n\n\n\ndf_raw[[\"start_date\"]]\n\n [1] 44208 44215 44230 44245 44255 44259 44264 44999 44276 44278 44297 44308\n[13] 44313 44318 44324    NA 44329 44332 44346 44358 44370 44383 44391 44397\n[25] 44412 44425 44434 44444 44461 44475 44500\n\n\n\n\n\n\ndf_clean &lt;- df_raw |&gt;\n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    ethnic_clean = case_when(\n      ethnic %in%  c(\"hispanic\", \"Hispanic\", \"hispamnic\") ~ \"hispanic\",\n      ethnic %in%  c(\"NOT hispanic\", \"not hispanic\") ~ \"not hispanic\",\n    ),\n    end_na_clean = na_if(end_na, -99),\n    end_emo_clean = na_if(end_emo, \"not done\") |&gt; as.numeric(),\n    start_date_clean = janitor::convert_to_date(start_date)\n  ) \n\n\n\n\n\n\ndf_clean |&gt; \n  select(start_date, start_date_clean) |&gt; \n  glimpse()\n\nRows: 30\nColumns: 2\n$ start_date       &lt;dbl&gt; 44208, 44215, 44230, 44245, 44255, 44259, 44264, 4499â€¦\n$ start_date_clean &lt;date&gt; 2021-01-12, 2021-01-19, 2021-02-03, 2021-02-18, 2021â€¦\n\n\n\n\ndf_clean |&gt; \n  count(start_date, start_date_clean) \n\n# A tibble: 30 Ã— 3\n   start_date start_date_clean     n\n        &lt;dbl&gt; &lt;date&gt;           &lt;int&gt;\n 1      44208 2021-01-12           1\n 2      44215 2021-01-19           1\n 3      44230 2021-02-03           1\n 4      44245 2021-02-18           1\n 5      44255 2021-02-28           1\n 6      44259 2021-03-04           1\n 7      44264 2021-03-09           1\n 8      44276 2021-03-21           1\n 9      44278 2021-03-23           1\n10      44297 2021-04-11           1\n# â„¹ 20 more rows"
  },
  {
    "objectID": "slides/index.html#check",
    "href": "slides/index.html#check",
    "title": "Data Cleaning",
    "section": "Check",
    "text": "Check\n\ndf_raw\n\n# A tibble: 31 Ã— 38\n   pat_id treatment start_date ethnic   race  dob                 ...7  start_bp\n   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;dttm&gt;              &lt;lgl&gt; &lt;chr&gt;   \n 1 001    upa            44208 hispanic Caucâ€¦ 2005-01-07 00:00:00 NA    114/72  \n 2 002    uste           44215 not hisâ€¦ Caucâ€¦ 1937-04-13 00:00:00 NA    132/86  \n 3 003    oza            44230 not hisâ€¦ Afriâ€¦ 1946-06-06 00:00:00 NA    124/92  \n 4 004    upa            44245 not hisâ€¦ Caucâ€¦ 1963-07-14 00:00:00 NA    144/83  \n 5 005    oza            44255 not hisâ€¦ Mixed 1978-05-12 00:00:00 NA    122/78  \n 6 006    uste           44259 not hisâ€¦ Other 1992-04-03 00:00:00 NA    121/80  \n 7 007    uste           44264 not hisâ€¦ Asian 1955-08-22 00:00:00 NA    133/74  \n 8 008    oza            44999 Hispanic afroâ€¦ 1974-09-11 00:00:00 NA    116/73  \n 9 009    upa            44276 NOT hisâ€¦ Caucâ€¦ 1984-11-14 00:00:00 NA    118/66  \n10 010    oza            44278 hispamnâ€¦ Caucâ€¦ 1972-12-20 00:00:00 NA    122/78  \n# â„¹ 21 more rows\n# â„¹ 30 more variables: `pre/post_wt_kg` &lt;chr&gt;, start_mes &lt;dbl&gt;,\n#   start_bss &lt;dbl&gt;, `Start Abd score` &lt;dbl&gt;, start_sys &lt;dbl&gt;,\n#   start_coping &lt;dbl&gt;, start_emo &lt;dbl&gt;,\n#   `Daily Life impact score at start` &lt;dbl&gt;, start_wbc &lt;dbl&gt;, start_plt &lt;chr&gt;,\n#   start_na &lt;chr&gt;, start_k &lt;chr&gt;, end_month &lt;dbl&gt;, end_day &lt;dbl&gt;,\n#   end_year &lt;dbl&gt;, end_mes &lt;dbl&gt;, end_bss &lt;dbl&gt;, end_abd &lt;dbl&gt;, â€¦"
  },
  {
    "objectID": "slides/index.html#what-is-the-unit-of-analysis",
    "href": "slides/index.html#what-is-the-unit-of-analysis",
    "title": "Data Cleaning",
    "section": "What is the Unit of Analysis?",
    "text": "What is the Unit of Analysis?\n\nWhile the unit of observation may be straightforward for outpatient visits, and complicated for inpatient stays, in the end we need to select a Unit of Analysis\nThis Unit of Analysis usually depends on the Question we want to ask"
  },
  {
    "objectID": "slides/index.html#is-the-unit-of-analysis-the-patient",
    "href": "slides/index.html#is-the-unit-of-analysis-the-patient",
    "title": "Data Cleaning",
    "section": "Is the Unit of Analysis the Patient?",
    "text": "Is the Unit of Analysis the Patient?\n\nDid the patient die?\nDid the patient have the outcome of colectomy?\nDid the patient reach disease remission?"
  },
  {
    "objectID": "slides/index.html#is-the-unit-of-analysis-the-visitencounter",
    "href": "slides/index.html#is-the-unit-of-analysis-the-visitencounter",
    "title": "Data Cleaning",
    "section": "Is the Unit of Analysis the Visit/Encounter?",
    "text": "Is the Unit of Analysis the Visit/Encounter?\n\nOften these are within-patient outcomes\n\nDid the C-reactive protein improve from Week 0 to Week 8?\nDid the number of sickle cell crises/year decrease after CRISPR gene therapy?\nDid the endoscopic ulcer score decrease on the experimental therapy vs placebo?"
  },
  {
    "objectID": "slides/index.html#reshaping-your-data-with-tidyr-1",
    "href": "slides/index.html#reshaping-your-data-with-tidyr-1",
    "title": "Data Cleaning",
    "section": "Reshaping your data with tidyr",
    "text": "Reshaping your data with tidyr\n\nR (and most R functions) are vectorized to handle tall data\nOne small observation per row\nLots of analyses in R are easier with tall data\n\n[Image]"
  },
  {
    "objectID": "slides/index.html#pivoting-longer",
    "href": "slides/index.html#pivoting-longer",
    "title": "Data Cleaning",
    "section": "Pivoting Longer",
    "text": "Pivoting Longer\n\nArguments: data, cols, names_to, values_to, and many optional arguments\nDetails from the tidyverse help page are here\ndata is your dataframe/tibble - you can pipe this in\ncols = columns to pivot, as a vector of names, or by number, or selected with tidyselect functions\nnames_to = A character vector specifying the new column or columns to create from the information stored in the column names of data specified by cols.\nvalues_to = A string specifying the name of the column to create from the data stored in cell values."
  },
  {
    "objectID": "slides/index.html#pivoting-longer-example",
    "href": "slides/index.html#pivoting-longer-example",
    "title": "Data Cleaning",
    "section": "Pivoting Longer (Example)",
    "text": "Pivoting Longer (Example)\nLetâ€™s start with the wide version (selected columns from messy_uc)\n\n\n# A tibble: 30 Ã— 8\n   pat_id treatment start_mes end_mes start_bss end_bss start_emo end_emo\n   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 001    upa               3       0      75.5    9.46      73.3    32.0\n 2 002    uste              2       1      53.6   31.6       55.7    43.2\n 3 003    oza               1       1      25.5   25.5       36.6    36.9\n 4 004    upa               3       1      79.4   35.4       80.8    58.7\n 5 005    oza               3       2      79.5   57.5       72.5    58.9\n 6 006    uste              2       1      54.5   32.5       48.3    34.6\n 7 007    uste              3       2      79.0   57.0       72.7    63.3\n 8 008    oza               3       2      79.0   57.0       74.6    65.3\n 9 009    upa               2       0      53.1    9.06      54.2    28.0\n10 010    oza               3       1      77.4   33.4       74.4    46.8\n# â„¹ 20 more rows\n\n\n\nNote that there are 30 rows, one per patient, with 6 measured quantities for each patient."
  },
  {
    "objectID": "slides/index.html#pivoting-longer-example-1",
    "href": "slides/index.html#pivoting-longer-example-1",
    "title": "Data Cleaning",
    "section": "Pivoting Longer (Example)",
    "text": "Pivoting Longer (Example)\nThis is the tall version we want to end up with.\n\n\n# A tibble: 180 Ã— 4\n   pat_id treatment measure   score\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 001    upa       start_mes  3   \n 2 001    upa       end_mes    0   \n 3 001    upa       start_bss 75.5 \n 4 001    upa       end_bss    9.46\n 5 001    upa       start_emo 73.3 \n 6 001    upa       end_emo   32.0 \n 7 002    uste      start_mes  2   \n 8 002    uste      end_mes    1   \n 9 002    uste      start_bss 53.6 \n10 002    uste      end_bss   31.6 \n# â„¹ 170 more rows\n\n\n\nNote that there now 180 rows (30*6), with one row per observation measure."
  },
  {
    "objectID": "slides/index.html#pivoting-longer-in-action",
    "href": "slides/index.html#pivoting-longer-in-action",
    "title": "Data Cleaning",
    "section": "Pivoting Longer In Action",
    "text": "Pivoting Longer In Action\n\nProblem: WideCode: pivot_longerResult: Tall\n\n\n\n\n# A tibble: 30 Ã— 8\n   pat_id treatment start_mes end_mes start_bss end_bss start_emo end_emo\n   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 001    upa               3       0      75.5    9.46      73.3    32.0\n 2 002    uste              2       1      53.6   31.6       55.7    43.2\n 3 003    oza               1       1      25.5   25.5       36.6    36.9\n 4 004    upa               3       1      79.4   35.4       80.8    58.7\n 5 005    oza               3       2      79.5   57.5       72.5    58.9\n 6 006    uste              2       1      54.5   32.5       48.3    34.6\n 7 007    uste              3       2      79.0   57.0       72.7    63.3\n 8 008    oza               3       2      79.0   57.0       74.6    65.3\n 9 009    upa               2       0      53.1    9.06      54.2    28.0\n10 010    oza               3       1      77.4   33.4       74.4    46.8\n# â„¹ 20 more rows\n\n\n\n\n\nwide |&gt; \n  pivot_longer(\n    cols = \"start_mes\":\"end_emo\", \n    # could also use 3:8\n               names_to = \"measure\",\n               values_to = \"score\")\n\n\n\n\n\n# A tibble: 180 Ã— 4\n   pat_id treatment measure   score\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 001    upa       start_mes  3   \n 2 001    upa       end_mes    0   \n 3 001    upa       start_bss 75.5 \n 4 001    upa       end_bss    9.46\n 5 001    upa       start_emo 73.3 \n 6 001    upa       end_emo   32.0 \n 7 002    uste      start_mes  2   \n 8 002    uste      end_mes    1   \n 9 002    uste      start_bss 53.6 \n10 002    uste      end_bss   31.6 \n# â„¹ 170 more rows"
  },
  {
    "objectID": "slides/index.html#pivoting-longer-1",
    "href": "slides/index.html#pivoting-longer-1",
    "title": "Data Cleaning",
    "section": "Pivoting Longer",
    "text": "Pivoting Longer\n\nYour Turn (Exercise) with endo_data\nMeasurements of Trans-Epithelial Electrical Resistance (TEER, the inverse of leakiness) were taken from biopsies of 3 segments of intestine.\nThis might be affected by portal hypertension in patients with liver cirrhosis\n\n\n\n# A tibble: 10 Ã— 5\n   pat_id portal_htn duod_teer ileal_teer colon_teer\n    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1      1          1      4.33       14.6       16.2\n 2      2          0     11.7        16.0       19.0\n 3      3          1      4.12       13.8       15.2\n 4      4          1      4.62       16.4       18.1\n 5      5          0     12.4        15.8       19.0\n 6      6          0     13.0        16.2       18.8\n 7      7          0     11.9        15.7       18.3\n 8      8          1      4.87       16.6       18.8\n 9      9          1      4.23       15.0       16.9\n10     10          0     12.8        16.7       19.1"
  },
  {
    "objectID": "slides/index.html#pivoting-wider-1",
    "href": "slides/index.html#pivoting-wider-1",
    "title": "Data Cleaning",
    "section": "Pivoting Wider",
    "text": "Pivoting Wider\n\nReverse Your Turn (Exercise)"
  },
  {
    "objectID": "slides/index.html#padding-unobvserved-times",
    "href": "slides/index.html#padding-unobvserved-times",
    "title": "Data Cleaning",
    "section": "Padding unobvserved times",
    "text": "Padding unobvserved times"
  },
  {
    "objectID": "slides/index.html#pivoting-longer-example-2",
    "href": "slides/index.html#pivoting-longer-example-2",
    "title": "Data Cleaning",
    "section": "Pivoting Longer (Example)",
    "text": "Pivoting Longer (Example)\nWhat do we want for these arguments?\n\ncols\nnames_to\nvalues_to\n\n\n\n# A tibble: 30 Ã— 8\n   pat_id treatment start_mes end_mes start_bss end_bss start_emo end_emo\n   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 001    upa               3       0      75.5    9.46      73.3    32.0\n 2 002    uste              2       1      53.6   31.6       55.7    43.2\n 3 003    oza               1       1      25.5   25.5       36.6    36.9\n 4 004    upa               3       1      79.4   35.4       80.8    58.7\n 5 005    oza               3       2      79.5   57.5       72.5    58.9\n 6 006    uste              2       1      54.5   32.5       48.3    34.6\n 7 007    uste              3       2      79.0   57.0       72.7    63.3\n 8 008    oza               3       2      79.0   57.0       74.6    65.3\n 9 009    upa               2       0      53.1    9.06      54.2    28.0\n10 010    oza               3       1      77.4   33.4       74.4    46.8\n# â„¹ 20 more rows"
  },
  {
    "objectID": "slides/index.html#one-minor-issue",
    "href": "slides/index.html#one-minor-issue",
    "title": "Data Cleaning",
    "section": "One Minor Issue",
    "text": "One Minor Issue\n\nProblemCodeResult\n\n\n\nthe â€œmeasureâ€ column combines a timepoint and the measure\nNeeds to be separated.\nYou already know how to use separate()\nArguments\n\ncol\nsep\ninto\n\n\n\n\n\ntall |&gt; \n  separate(col = \"measure\",\n           sep = \"_\",\n           into = c(\"timept\", \"measure\"))\n\n\n\n\n\n# A tibble: 180 Ã— 5\n   pat_id treatment timept measure score\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n 1 001    upa       start  mes      3   \n 2 001    upa       end    mes      0   \n 3 001    upa       start  bss     75.5 \n 4 001    upa       end    bss      9.46\n 5 001    upa       start  emo     73.3 \n 6 001    upa       end    emo     32.0 \n 7 002    uste      start  mes      2   \n 8 002    uste      end    mes      1   \n 9 002    uste      start  bss     53.6 \n10 002    uste      end    bss     31.6 \n# â„¹ 170 more rows"
  },
  {
    "objectID": "slides/index.html#one-minor-issue---separation-of-measure",
    "href": "slides/index.html#one-minor-issue---separation-of-measure",
    "title": "Data Cleaning",
    "section": "One Minor Issue - Separation of measure",
    "text": "One Minor Issue - Separation of measure\n\nProblemCodeResultAlternative within pivot_longer\n\n\n\nthe â€œmeasureâ€ column combines a timepoint and the measure\nNeeds to be separated.\nYou already know how to use separate()\nArguments\n\ncol\nsep\ninto\n\n\n\n\n\ntall |&gt; \n  separate(col = \"measure\",\n           sep = \"_\",\n           into = c(\"timept\", \"measure\"))\n\n\n\n\n\n# A tibble: 180 Ã— 5\n   pat_id treatment timept measure score\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n 1 001    upa       start  mes      3   \n 2 001    upa       end    mes      0   \n 3 001    upa       start  bss     75.5 \n 4 001    upa       end    bss      9.46\n 5 001    upa       start  emo     73.3 \n 6 001    upa       end    emo     32.0 \n 7 002    uste      start  mes      2   \n 8 002    uste      end    mes      1   \n 9 002    uste      start  bss     53.6 \n10 002    uste      end    bss     31.6 \n# â„¹ 170 more rows\n\n\n\n\n\nYou can do this within pivot_longer with one more argument (if you are fancy)\n\n\nwide |&gt; \n  pivot_longer(cols = 3:8,\n    names_to = c(\"timept\", \"measure\"),\n    names_sep = \"_\",\n    values_to = \"score\")\n\n# A tibble: 180 Ã— 5\n   pat_id treatment timept measure score\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n 1 001    upa       start  mes      3   \n 2 001    upa       end    mes      0   \n 3 001    upa       start  bss     75.5 \n 4 001    upa       end    bss      9.46\n 5 001    upa       start  emo     73.3 \n 6 001    upa       end    emo     32.0 \n 7 002    uste      start  mes      2   \n 8 002    uste      end    mes      1   \n 9 002    uste      start  bss     53.6 \n10 002    uste      end    bss     31.6 \n# â„¹ 170 more rows"
  },
  {
    "objectID": "slides/index.html#doing-the-pivot_longer",
    "href": "slides/index.html#doing-the-pivot_longer",
    "title": "Data Cleaning",
    "section": "Doing the pivot_longer()",
    "text": "Doing the pivot_longer()\nWhat values do we want for these key arguments in order to pivot_longer?\n\ncols (which columns to pivot)\nnames_to (variable to store the names)\nvalues_to (variable to store the values)\n\n\n\n# A tibble: 30 Ã— 8\n   pat_id treatment start_mes end_mes start_bss end_bss start_emo end_emo\n   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 001    upa               3       0      75.5    9.46      73.3    32.0\n 2 002    uste              2       1      53.6   31.6       55.7    43.2\n 3 003    oza               1       1      25.5   25.5       36.6    36.9\n 4 004    upa               3       1      79.4   35.4       80.8    58.7\n 5 005    oza               3       2      79.5   57.5       72.5    58.9\n 6 006    uste              2       1      54.5   32.5       48.3    34.6\n 7 007    uste              3       2      79.0   57.0       72.7    63.3\n 8 008    oza               3       2      79.0   57.0       74.6    65.3\n 9 009    upa               2       0      53.1    9.06      54.2    28.0\n10 010    oza               3       1      77.4   33.4       74.4    46.8\n# â„¹ 20 more rows"
  },
  {
    "objectID": "slides/index.html#pivoting-longer-with-endo_data",
    "href": "slides/index.html#pivoting-longer-with-endo_data",
    "title": "Data Cleaning",
    "section": "Pivoting Longer with endo_data",
    "text": "Pivoting Longer with endo_data\n\nThe DatasetThe ArgumentsThe CodeThe Solution\n\n\n\n\n# A tibble: 10 Ã— 5\n   pat_id portal_htn duod_teer ileal_teer colon_teer\n    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1      1          1      4.33       14.6       16.2\n 2      2          0     11.7        16.0       19.0\n 3      3          1      4.12       13.8       15.2\n 4      4          1      4.62       16.4       18.1\n 5      5          0     12.4        15.8       19.0\n 6      6          0     13.0        16.2       18.8\n 7      7          0     11.9        15.7       18.3\n 8      8          1      4.87       16.6       18.8\n 9      9          1      4.23       15.0       16.9\n10     10          0     12.8        16.7       19.1\n\n\n\n\n\nWhat values do you want to use for:\n\ncols\nnames_pattern = â€œ(.+)_teerâ€\nnames_to\nvalues_to\n\nNote that we are giving you the value for names_pattern, which means that we want to keep the characters of the name (of whatever length) before â€œ_teerâ€\n\n\n\n\nFill in the blanks to pivot this dataset to tall format, with columns for the intestinal location and the teer value.\nNote that we are giving you names_pattern\n\n\nendo_data |&gt; \n  pivot_longer(\n    cols =  ,\n    names_pattern = \"(.+)_teer\",\n    names_to =   ,\n    values_to = \n  )\n\n\n\n\nFill in the blanks to pivot this dataset to tall format, with columns for the intestinal location and the teer value.\n\n\nendo_data |&gt; \n  pivot_longer(\n    cols = \"duod_teer\":\"colon_teer\",\n    names_pattern = \"(.+)_teer\",\n    names_to = c(\"location\"),\n    values_to = \"teer\"\n  )\n\n\nRun the code, and look at the resulting table.\nDo you think that portal hypertension has an effect on TEER and (its inverse) epithelial leakiness?"
  },
  {
    "objectID": "slides/index.html#thickening-time-to-a-usable-level",
    "href": "slides/index.html#thickening-time-to-a-usable-level",
    "title": "Data Cleaning",
    "section": "Thickening Time to a Usable Level",
    "text": "Thickening Time to a Usable Level\n\nGoalCodeResult\n\n\n\nThe thicken function adds a column to a data frame that is of a higher interval than the original variable.\nThe variable time_stamp has the interval of seconds\nWe can thicken the data to day, or to week, or to month.\nThen we can count events by a usable unit of time\n\n\n\n\nWe will thicken to month\nThen count overdoses by month\n\n\nemergency |&gt; \n  thicken('month') |&gt; \n  filter(str_detect(title, \"OVERDOSE\")) |&gt;   group_by(time_stamp_month) |&gt; \n  mutate(ods = sum(str_detect(title, \"OVERDOSE\"))) |&gt; \n    select(time_stamp_month, ods) |&gt; \n  distinct()\n\n\n\n\nThis lets us count events like overdoses by month with time_stamp_month.\n\n\n\n# A tibble: 11 Ã— 2\n# Groups:   time_stamp_month [11]\n   time_stamp_month   ods\n   &lt;date&gt;           &lt;int&gt;\n 1 2015-12-01          91\n 2 2016-01-01         135\n 3 2016-02-01         141\n 4 2016-03-01         161\n 5 2016-04-01         124\n 6 2016-05-01         149\n 7 2016-06-01         140\n 8 2016-07-01         147\n 9 2016-08-01         137\n10 2016-09-01         171\n11 2016-10-01          95"
  },
  {
    "objectID": "slides/index.html#padding-unobserved-times",
    "href": "slides/index.html#padding-unobserved-times",
    "title": "Data Cleaning",
    "section": "Padding Unobserved Times",
    "text": "Padding Unobserved Times\n\nThe ProblemThe CodeThe Result\n\n\n\nWe can fill in (pad) the unobserved weekend days with the pad() function.\n\n\n\n\nfcp |&gt; \n  pad(group = \"pat_id\") |&gt; \n  tidyr::fill(pat_id) |&gt; # fill in the missing pat_ids\n  print(n = 14)\n\n\n\n\n\n\n\n# A tibble: 21 Ã— 3\n   pat_id date         fcp\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;\n 1 001    2022-12-01  1574\n 2 001    2022-12-02  1323\n 3 001    2022-12-03    NA\n 4 001    2022-12-04    NA\n 5 001    2022-12-05   673\n 6 001    2022-12-06   314\n 7 001    2022-12-07   168\n 8 002    2022-11-30  1393\n 9 002    2022-12-01  1014\n10 002    2022-12-02   812\n11 002    2022-12-03    NA\n12 002    2022-12-04    NA\n13 002    2022-12-05   247\n14 002    2022-12-06   118\n# â„¹ 7 more rows\n\n\n\n\nNew observations are created on the missing dates\nNAs are filled in for the missing FCPs, with one for each day and group (pat_id)\nwe used tidyr::fill(pat_id) to fill in the missing pat_ids"
  },
  {
    "objectID": "slides/index.html#local-demographics-with-cdc-svi-data",
    "href": "slides/index.html#local-demographics-with-cdc-svi-data",
    "title": "Data Cleaning",
    "section": "Local Demographics with CDC SVI data",
    "text": "Local Demographics with CDC SVI data\n\nThe ProblemThe DataThe CodeThe Result\n\n\n\n\n\nWe have 2 datasets, one local Demographics and Census Tract, and one from the CDC that has values for Social Vulnerability Index by Census Tract\nWe want to know the median SVI for the neighborhood of each patient\nWe need to left_join these datasets together by matching on the Census Tract\n\n\n\n\n\nJoins\n\n\n\n\n\n\n\nWhat is the common uniqid/key?\n\n\n\n\ndemo\n\n\n\n# A tibble: 9 Ã— 4\n  pat_id name                 htn census_tract\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n1 001    Arthur Blankenship     0  26161404400\n2 002    Britney Jonas          0  26161405100\n3 003    Sally Davis            1  26161402100\n4 004    Al Jones               0  26161403200\n5 005    Gary Hamill            1  26161405200\n6 006    Ken Bartoletti         0  26161404500\n7 007    Ike Gerhold            0  26161405600\n8 008    Tatiana Grant          0  26161404300\n9 009    Antione Delacroix      1  26161405500\n\n\n\n\ncdc\n\n\n\n# A tibble: 9 Ã— 2\n  census_tract   svi\n         &lt;dbl&gt; &lt;dbl&gt;\n1  26161404400  0.12\n2  26161405100  0.67\n3  26161402100  0.43\n4  26161403200  0.07\n5  26161405200  0.71\n6  26161404500  0.23\n7  26161405600  0.27\n8  26161404300  0.21\n9  26161405500  0.62\n\n\n\n\n\n\n\nReplace the arguments to join the demographic data (demo) on the left to add Social Vulnerability index (svi) from the cdc dataset on the RHS. Left join by census tract.\n\n\nleft_join(dataset_x, dataset_y, by = \"key\")\n\n\n\n\nIs there an association between social vulnerability and hypertension?\n\n\n\n# A tibble: 9 Ã— 5\n  pat_id name                 htn census_tract   svi\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 001    Arthur Blankenship     0  26161404400  0.12\n2 002    Britney Jonas          0  26161405100  0.67\n3 003    Sally Davis            1  26161402100  0.43\n4 004    Al Jones               0  26161403200  0.07\n5 005    Gary Hamill            1  26161405200  0.71\n6 006    Ken Bartoletti         0  26161404500  0.23\n7 007    Ike Gerhold            0  26161405600  0.27\n8 008    Tatiana Grant          0  26161404300  0.21\n9 009    Antione Delacroix      1  26161405500  0.62"
  },
  {
    "objectID": "slides/index.html#pipes",
    "href": "slides/index.html#pipes",
    "title": "Data Cleaning",
    "section": "Pipes",
    "text": "Pipes\n\n2014+ magrittr pipe %&gt;%\n2021+ (R \\(\\geq\\) 4.1.0) native R pipe |&gt;\n\n2022 Isabella VelÃ¡squez Understanding the native R pipe |&gt; https://ivelasq.rbind.io/blog/understanding-the-r-pipe/\n\n\n\n\n\nwhatever(arg1, arg2, arg3, ...)\n\narg1 |&gt;  \n  whatever(arg2, arg3)\n\n\n\n\n\nmean(0:10)\n\n0:10 |&gt; \n  mean()\n\n\n\n\nChange CTRL + Shift + M shortcut to native pipe:\nTools -&gt; Global Options -&gt; Code -&gt;\nÂ Â  Editing -&gt; check Use Native Pipe Operator"
  },
  {
    "objectID": "slides/index.html#r-for-data-science-ch-18-pipes",
    "href": "slides/index.html#r-for-data-science-ch-18-pipes",
    "title": "Data Cleaning",
    "section": "R for Data Science: Ch 18 Pipes",
    "text": "R for Data Science: Ch 18 Pipes\n\n\nhttps://r4ds.had.co.nz/pipes.html#pipes"
  },
  {
    "objectID": "slides/index.html#namespacing",
    "href": "slides/index.html#namespacing",
    "title": "Data Cleaning",
    "section": "Namespacing",
    "text": "Namespacing\ndplyr::select()\n\ntells R explicitly to use the function select from the package dplyr\ncan help to avoid name conflicts (e.g., MASS::select())\ndoes not require library(dplyr)\n\n\n\n\n\nlibrary(dplyr)\n\nselect(mtcars, mpg, cyl) \n\nmtcars |&gt;  \n  select(mpg, cyl) \n\n\n\n\n\n# library(dplyr) not needed\n\ndplyr::select(mtcars, mpg, cyl) \n\nmtcars |&gt;  \n  dplyr::select(mpg, cyl)"
  },
  {
    "objectID": "exercises.html#set-up",
    "href": "exercises.html#set-up",
    "title": "Data Cleaning: Exercises",
    "section": "Set up",
    "text": "Set up\n\n# import raw data\ndf_raw &lt;- read_excel(\n  path = here(\"data\", \"messy_uc.xlsx\"),\n  sheet = \"Data\",\n  skip = 5\n)\n\n# do initial cleaning of variable names and removing empty rows/columns\ndf_clean &lt;- df_raw |&gt; \n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\"))"
  },
  {
    "objectID": "exercises.html#sp1",
    "href": "exercises.html#sp1",
    "title": "Data Cleaning: Exercises",
    "section": "SP1",
    "text": "SP1\n\nExplore the values of race.\n\n\n\nShow the code solution\ndf_clean |&gt; count(race)\n\n\n# A tibble: 9 Ã— 2\n  race                 n\n  &lt;chr&gt;            &lt;int&gt;\n1 African-American     4\n2 AmerInd              1\n3 Asian                1\n4 Caucasian           17\n5 H/API                1\n6 Hawaiian             1\n7 Mixed                1\n8 Other                2\n9 afromerican          2\n\n\n\nIn the df_clean data set, create a new variable named race_clean that cleans the coding of race (combine â€œAfrican-Americanâ€ & â€œafromericanâ€; â€œH/APIâ€ & â€œMixedâ€ & â€œOtherâ€).\n\n\n\nShow the code solution\ndf_clean &lt;- df_raw |&gt; \n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    race_clean = case_when(\n      race %in% c(\"African-American\", \"afromerican\") ~ \"African-American\",\n      race %in% c(\"H/API\", \"Mixed\", \"Other\") ~ \"Other\",\n      TRUE ~ race\n    )\n  )\n\n\n\nConfirm the new race_clean variable is coded correctly.\n\n\n\nShow the code solution\ndf_clean |&gt; \n  count(race_clean, race)\n\n\n# A tibble: 9 Ã— 3\n  race_clean       race                 n\n  &lt;chr&gt;            &lt;chr&gt;            &lt;int&gt;\n1 African-American African-American     4\n2 African-American afromerican          2\n3 AmerInd          AmerInd              1\n4 Asian            Asian                1\n5 Caucasian        Caucasian           17\n6 Hawaiian         Hawaiian             1\n7 Other            H/API                1\n8 Other            Mixed                1\n9 Other            Other                2"
  },
  {
    "objectID": "exercises.html#sp2",
    "href": "exercises.html#sp2",
    "title": "Data Cleaning: Exercises",
    "section": "SP2",
    "text": "SP2\n\nExplore the type of and values of start_plt.\n\n\n\nShow the code solution\ndf_clean |&gt; \n  count(start_plt)\n\n\n# A tibble: 28 Ã— 2\n   start_plt        n\n   &lt;chr&gt;        &lt;int&gt;\n 1 115K/microL      1\n 2 1550K/microL     1\n 3 177K/microL      1\n 4 188K/microL      1\n 5 197K/microL      1\n 6 204K/microL      1\n 7 249K/microL      1\n 8 258K/microL      1\n 9 273K/microL      1\n10 288K/microL      1\n# â„¹ 18 more rows\n\n\nShow the code solution\ndf_clean |&gt; \n  select(start_plt) |&gt; \n  glimpse()\n\n\nRows: 30\nColumns: 1\n$ start_plt &lt;chr&gt; \"273K/microL\", \"414K/microL\", \"323K/microL\", \"389K/microL\", â€¦\n\n\nShow the code solution\ndf_clean[[\"start_plt\"]]\n\n\n [1] \"273K/microL\"  \"414K/microL\"  \"323K/microL\"  \"389K/microL\"  \"411K/microL\" \n [6] \"427K/microL\"  \"249K/microL\"  \"197K/microL\"  \"204K/microL\"  \"305K/microL\" \n[11] \"347K/microL\"  \"402K/microL\"  \"389K/microL\"  \"432K/microL\"  \"288K/microL\" \n[16] \"177K/microL\"  \"290K/microL\"  \"312K/microL\"  \"399K/microL\"  \"423K/microL\" \n[21] \"clumped\"      \"323K/microL\"  \"258K/microL\"  \"115K/microL\"  \"1550K/microL\"\n[26] \"37K/microL\"   \"188K/microL\"  \"456K/microL\"  \"356K/microL\"  \"291K/microL\" \n\n\n\nIn the df_clean data set, create a new variable named start_plt_clean that corrects any unusual values and assigns the correct variable type.\n\n\n\nShow the code solution\ndf_clean &lt;- df_raw |&gt; \n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    race_clean = case_when(\n      race %in% c(\"African-American\", \"afromerican\") ~ \"African-American\",\n      race %in% c(\"H/API\", \"Mixed\", \"Other\") ~ \"Other\",\n      TRUE ~ race\n    ),\n    start_plt_clean = parse_number(start_plt) \n  )\n\n\n\nConfirm the new start_plt_clean variable is coded correctly.\n\n\n\nShow the code solution\ndf_clean |&gt; \n  count(start_plt_clean, start_plt)\n\n\n# A tibble: 28 Ã— 3\n   start_plt_clean start_plt       n\n             &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n 1              37 37K/microL      1\n 2             115 115K/microL     1\n 3             177 177K/microL     1\n 4             188 188K/microL     1\n 5             197 197K/microL     1\n 6             204 204K/microL     1\n 7             249 249K/microL     1\n 8             258 258K/microL     1\n 9             273 273K/microL     1\n10             288 288K/microL     1\n# â„¹ 18 more rows\n\n\nShow the code solution\ndf_clean |&gt; \n  select(start_plt, start_plt_clean) |&gt; \n  glimpse()\n\n\nRows: 30\nColumns: 2\n$ start_plt       &lt;chr&gt; \"273K/microL\", \"414K/microL\", \"323K/microL\", \"389K/micâ€¦\n$ start_plt_clean &lt;dbl&gt; 273, 414, 323, 389, 411, 427, 249, 197, 204, 305, 347,â€¦\n\n\nShow the code solution\ndf_clean[[\"start_plt_clean\"]]\n\n\n [1]  273  414  323  389  411  427  249  197  204  305  347  402  389  432  288\n[16]  177  290  312  399  423   NA  323  258  115 1550   37  188  456  356  291\nattr(,\"problems\")\n# A tibble: 1 Ã— 4\n    row   col expected actual \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  \n1    21    NA a number clumped"
  },
  {
    "objectID": "slides/index.html#scope",
    "href": "slides/index.html#scope",
    "title": "Data Cleaning",
    "section": "Scope",
    "text": "Scope\n\nTaming the Data Beast, by Allison HorstTaming the Data Beast, from Allison Horstâ€™s Data Science Illustrations"
  },
  {
    "objectID": "exercises.html#cl1",
    "href": "exercises.html#cl1",
    "title": "Data Cleaning: Exercises",
    "section": "CL1",
    "text": "CL1"
  },
  {
    "objectID": "exercises.html#cl2",
    "href": "exercises.html#cl2",
    "title": "Data Cleaning: Exercises",
    "section": "CL2",
    "text": "CL2"
  },
  {
    "objectID": "exercises.html#sp3",
    "href": "exercises.html#sp3",
    "title": "Data Cleaning: Exercises",
    "section": "SP3",
    "text": "SP3\n\nExplore the type of and values of race_clean.\n\n\n\nShow the code solution\ndf_clean |&gt; \n  count(race_clean)\n\n\n# A tibble: 6 Ã— 2\n  race_clean           n\n  &lt;chr&gt;            &lt;int&gt;\n1 African-American     6\n2 AmerInd              1\n3 Asian                1\n4 Caucasian           17\n5 Hawaiian             1\n6 Other                4\n\n\n\nConvert the race_clean variable to a factor such that the most common values present in order in a summary table.\n\n\n\nShow the code solution\ndf_clean &lt;- df_raw |&gt; \n  janitor::clean_names() |&gt; \n  janitor::remove_empty(which = c(\"rows\", \"cols\")) |&gt; \n  mutate(\n    race_clean = case_when(\n      race %in% c(\"African-American\", \"afromerican\") ~ \"African-American\",\n      race %in% c(\"H/API\", \"Mixed\", \"Other\") ~ \"Other\",\n      TRUE ~ race\n    ) |&gt; fct_infreq(),\n    start_plt_clean = parse_number(start_plt) \n  )\n\n\n\nConfirm the new coding of race_clean.\n\n\n\nShow the code solution\ndf_clean |&gt; \n  count(race_clean, race)\n\n\n# A tibble: 9 Ã— 3\n  race_clean       race                 n\n  &lt;fct&gt;            &lt;chr&gt;            &lt;int&gt;\n1 Caucasian        Caucasian           17\n2 African-American African-American     4\n3 African-American afromerican          2\n4 Other            H/API                1\n5 Other            Mixed                1\n6 Other            Other                2\n7 AmerInd          AmerInd              1\n8 Asian            Asian                1\n9 Hawaiian         Hawaiian             1\n\n\nShow the code solution\ndf_clean |&gt; \n  count(race_clean)\n\n\n# A tibble: 6 Ã— 2\n  race_clean           n\n  &lt;fct&gt;            &lt;int&gt;\n1 Caucasian           17\n2 African-American     6\n3 Other                4\n4 AmerInd              1\n5 Asian                1\n6 Hawaiian             1"
  },
  {
    "objectID": "exercises.html#ph1",
    "href": "exercises.html#ph1",
    "title": "Data Cleaning: Exercises",
    "section": "PH1",
    "text": "PH1"
  },
  {
    "objectID": "exercises.html#ph2",
    "href": "exercises.html#ph2",
    "title": "Data Cleaning: Exercises",
    "section": "PH2",
    "text": "PH2"
  },
  {
    "objectID": "slides/index.html#exercise",
    "href": "slides/index.html#exercise",
    "title": "Data Cleaning",
    "section": "Exercise",
    "text": "Exercise\n\nComplete Data Cleaning Fundamentals Exercise SP1.\n\nâ€“&gt; Take me to the exercises &lt;â€“\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/index.html#exercise-1",
    "href": "slides/index.html#exercise-1",
    "title": "Data Cleaning",
    "section": "Exercise",
    "text": "Exercise\n\nComplete Data Cleaning Fundamentals Exercise SP2.\n\nâ€“&gt; Take me to the exercises &lt;â€“\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/index.html#exercise-2",
    "href": "slides/index.html#exercise-2",
    "title": "Data Cleaning",
    "section": "Exercise",
    "text": "Exercise\n\nComplete Data Cleaning Fundamentals Exercise SP3.\n\nâ€“&gt; Take me to the exercises &lt;â€“\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/index.html#exercise-ph1",
    "href": "slides/index.html#exercise-ph1",
    "title": "Data Cleaning",
    "section": "Exercise PH1",
    "text": "Exercise PH1\n\nComplete Data Cleaning Fundamentals Exercise PH1.\n\nIf you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the â€œthumbs upâ€ emoji on your screen.\nIf you are having trouble, click on the Reactions tab in Zoom, and click to put the â€œraise handâ€ emoji on your screen.\n\nâ€“&gt; Take me to the exercises &lt;â€“\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/index.html#patient-demographics-with-lab-results-yout-turn-to-join",
    "href": "slides/index.html#patient-demographics-with-lab-results-yout-turn-to-join",
    "title": "Data Cleaning",
    "section": "Patient Demographics with Lab results (Yout Turn to Join)",
    "text": "Patient Demographics with Lab results (Yout Turn to Join)\n\n\n\nWe have some basic Patient Demographics in one table\n\n\n\n# A tibble: 9 Ã— 3\n  pat_id name                 age\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 001    Arthur Blankenship    67\n2 002    Britney Jonas         23\n3 003    Sally Davis           63\n4 004    Al Jones              44\n5 005    Gary Hamill           38\n6 006    Ken Bartoletti        33\n7 007    Ike Gerhold           52\n8 008    Tatiana Grant         42\n9 009    Antione Delacroix     27\n\n\n\nand potassium levels and creatinine levels in 2 other tables\n\n\n# A tibble: 6 Ã— 2\n  pat_id     k\n  &lt;chr&gt;  &lt;dbl&gt;\n1 001      3.2\n2 002      3.7\n3 003      4.2\n4 004      4.4\n5 005      4.1\n6 006      4  \n\n\n\n\n# A tibble: 6 Ã— 2\n  pat_id    cr\n  &lt;chr&gt;  &lt;dbl&gt;\n1 001      0.2\n2 002      0.5\n3 003      0.9\n4 004      1.5\n5 005      0.7\n6 006      0.9"
  },
  {
    "objectID": "slides/index.html#your-turn-to-join",
    "href": "slides/index.html#your-turn-to-join",
    "title": "Data Cleaning",
    "section": "Your Turn to Join",
    "text": "Your Turn to Join\n\nWe want to join the correct labs (9 rows each) to the correct patients.\nThe unique identifier (called the uniqid or key or recordID) is pat_id.\nIt only occurs once for each patient/row\nIt appears in each table we want to join\nThe pat_id is of the character type in each (a common downfall if one is character, one is numeric, but they look the same - but donâ€™t match)\nWe want to start with demographics, then add datasets that match to the right.\nWe will use demo as our base dataset on the left hand side (LHS), and first join the potassium (pot) results (RHS)"
  },
  {
    "objectID": "slides/index.html#your-turn-to-join-1",
    "href": "slides/index.html#your-turn-to-join-1",
    "title": "Data Cleaning",
    "section": "Your Turn to Join",
    "text": "Your Turn to Join\n\nThe ProblemThe CodeThe Result\n\n\n\nJoining demo to pot with a left_join\nleft_join(data_x, data_y, by = â€œuniqidâ€)\n\n\n\n\nreplace the arguments with the correct ones to join demo to pot and produce new_data.\n\n\nnew_data &lt;- left_join(data_x, data_y, by = \"uniqid\")\nnew_data\n\n\n\n\n\n# A tibble: 9 Ã— 4\n  pat_id name                 age     k\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 001    Arthur Blankenship    67   3.2\n2 002    Britney Jonas         23   3.7\n3 003    Sally Davis           63   4.2\n4 004    Al Jones              44   4.4\n5 005    Gary Hamill           38   4.1\n6 006    Ken Bartoletti        33   4  \n7 007    Ike Gerhold           52   3.6\n8 008    Tatiana Grant         42   4.2\n9 009    Antione Delacroix     27   4.9"
  },
  {
    "objectID": "slides/index.html#exercise-ph2",
    "href": "slides/index.html#exercise-ph2",
    "title": "Data Cleaning",
    "section": "Exercise PH2",
    "text": "Exercise PH2\n\nComplete Data Cleaning Fundamentals Exercise PH2.\nâ€“&gt; Take me to the exercises &lt;â€“\n\nIf you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the â€œthumbs upâ€ emoji on your screen.\nIf you are having trouble, click on the Reactions tab in Zoom, and click to put the â€œraise handâ€ emoji on your screen.\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/index.html#solution-ph2",
    "href": "slides/index.html#solution-ph2",
    "title": "Data Cleaning",
    "section": "Solution PH2",
    "text": "Solution PH2\n\nnew_data &lt;- left_join(demo, pot, by = \"pat_id\")\nnew_data"
  },
  {
    "objectID": "slides/index.html#now-add-creatinine-cr-to-new_data",
    "href": "slides/index.html#now-add-creatinine-cr-to-new_data",
    "title": "Data Cleaning",
    "section": "Now add Creatinine (cr) to new_data",
    "text": "Now add Creatinine (cr) to new_data\n\nThe ProblemThe CodeThe Result\n\n\n\nJoining new_data and cr with a left_join\nleft_join(data_x, data_y, by = â€œuniqidâ€)\n\n\n\n\nReplace the arguments with the correct ones to join new_data and cr and produce new_data2.\n\n\nnew_data2 &lt;- left_join(data_x, data_y, by = \"uniqid\")\nnew_data2\n\n\n\n\n\n# A tibble: 9 Ã— 5\n  pat_id name                 age     k    cr\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 001    Arthur Blankenship    67   3.2   0.2\n2 002    Britney Jonas         23   3.7   0.5\n3 003    Sally Davis           63   4.2   0.9\n4 004    Al Jones              44   4.4   1.5\n5 005    Gary Hamill           38   4.1   0.7\n6 006    Ken Bartoletti        33   4     0.9\n7 007    Ike Gerhold           52   3.6   0.7\n8 008    Tatiana Grant         42   4.2   1  \n9 009    Antione Delacroix     27   4.9   1.7"
  },
  {
    "objectID": "slides/index.html#exercise-ph3",
    "href": "slides/index.html#exercise-ph3",
    "title": "Data Cleaning",
    "section": "Exercise PH3",
    "text": "Exercise PH3\n\nComplete Data Cleaning Fundamentals Exercise PH3.\nâ€“&gt; Take me to the exercises &lt;â€“\n\nIf you have the exercise done correctly, click on the Reactions tab in Zoom, and click to put the â€œthumbs upâ€ emoji on your screen.\nIf you are having trouble, click on the Reactions tab in Zoom, and click to put the â€œraise handâ€ emoji on your screen.\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/index.html#patient-demographics-with-lab-results-your-turn-to-join",
    "href": "slides/index.html#patient-demographics-with-lab-results-your-turn-to-join",
    "title": "Data Cleaning",
    "section": "Patient Demographics with Lab results (Your Turn to Join)",
    "text": "Patient Demographics with Lab results (Your Turn to Join)\n\n\n\nWe have some basic Patient Demographics in one table\n\n\n\n# A tibble: 9 Ã— 3\n  pat_id name                 age\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 001    Arthur Blankenship    67\n2 002    Britney Jonas         23\n3 003    Sally Davis           63\n4 004    Al Jones              44\n5 005    Gary Hamill           38\n6 006    Ken Bartoletti        33\n7 007    Ike Gerhold           52\n8 008    Tatiana Grant         42\n9 009    Antione Delacroix     27\n\n\n\nand potassium levels and creatinine levels in 2 other tables\n\n\n# A tibble: 6 Ã— 2\n  pat_id     k\n  &lt;chr&gt;  &lt;dbl&gt;\n1 001      3.2\n2 002      3.7\n3 003      4.2\n4 004      4.4\n5 005      4.1\n6 006      4  \n\n\n\n\n# A tibble: 6 Ã— 2\n  pat_id    cr\n  &lt;chr&gt;  &lt;dbl&gt;\n1 001      0.2\n2 002      0.5\n3 003      0.9\n4 004      1.5\n5 005      0.7\n6 006      0.9"
  },
  {
    "objectID": "slides/index.html#other-fancy-joins",
    "href": "slides/index.html#other-fancy-joins",
    "title": "Data Cleaning",
    "section": "Other Fancy Joins",
    "text": "Other Fancy Joins\n\nleft_join is your workhorse. Start with patient identifiers/uniqid and add data to the right side.\nSometimes you will need to wrangle/process incoming data, then right_join it to the patient demographics\nThere are multiple kinds of fancy joins (semi_join, anti_join, inner_join, full_join) which come in handy once in a while.+\nThese are all well explained here"
  }
]