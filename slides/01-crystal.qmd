
```{r font-awesome-color}
#| echo: false

# fill for font awesome icons
fa_fill <- "#7090A5"

```

# Principles of Data Management


## Data Horror Stories

::: {.r-stack}
![](images/messy_spreadsheet1.PNG){.fragment width="850" height="550"}

![](images/messy_spreadsheet2.PNG){.fragment width="650" height="450"}

![](images/messy_spreadsheet3.PNG){.fragment width="550" height="250"}
:::


## Data Horror Stories

![](images/disappointment.png){fig-align="center" width=90%}

::: footer  
Image from knowyourmeme.com
:::  


## Data Organizing Principles

<br>

:::: {.columns}

::: {.column width="50%"}

- Data Structure
- Variable Values

:::

::: {.column width="50%"}

- Variable Types
- Missing Data

:::

::::

![](images/messy_data.PNG)


## Data Structure

1. Data should make a rectangle of rows and columns 
    - You should have the expected number of rows (cases in your data)
    - You should have the expected number of columns (variables in your data)

![](images/row_col.PNG){fig-align="center"}


## Data Structure

2. Variable names should be the first, and **only** the first, row of your data  
    - They should also adhere to best practices
    - Variable names should
      - Be unique
      - Be meaningful (`gender` instead of `X1`)
      - Not include spaces
      - Not include special characters except `_`
        - So no `/`, `-`, `!`, `"`
      - Not start with a number or special character


## Exercise

What data structure issues do you notice in our sample data?

![](images/messy_data.PNG){fig-align="center"}

```{r}
#| echo: false
#| cache: false
countdown(minutes = 1, font_size = "1em")
```


    
## Data Structure

- Variable names are not the first row of the data
- Our data does not make a rectangle - Empty column, empty rows
- Variable names do not adhere to best practices

![](images/structure.PNG){fig-align="center"}

## Variable Values

1. Values should be explicit, not implicit
    - If a blank cell is implied to be zero, fill that cell with an actual zero
    - No color coding should be used to indicate information. Make a new variable.

. . . 

2. Values should be analyzable. This means no more than one measure should be captured in a variable.

. . .

3. Variables should be captured consistently within a column
    - Dates captured consistently (i.e. [YYYY-MM-DD](https://www.iso.org/iso-8601-date-and-time-format.html))
    - Categories captured consistently (both spelling and capitalization)
    - If the variable is numeric, values should fall within your expected range


## Exercise


What variable value issues do you notice in our sample data?

![](images/messy_data.PNG){fig-align="center"}

```{r}
#| echo: false
#| cache: false
countdown(minutes = 1, font_size = "1em")
```

  
## Variable Values

- Color coding used to indicate information
- Two things measured in one column
- Categorical values captured inconsistently


![](images/values.PNG){fig-align="center"}

## Variable Types

Variables should be stored as your expected type (or in R terms - `class`)

. . .

1. **Numeric**
    - Contain numeric values (14.5, 14.539, 789, -24)
    - Numeric variables cannot contain special characters, spaces, or letters
      - 100mg
      - 83/150
      - " 89"

. . . 

2. **Date, Time, Date-Time**
    - Represented in R as either `<date>`, `<time>` or `<dttm>`/`<POSIXct>`
    - Allow you to perform calculations on dates


## Variable Types

3. **Character**
    - Contain character values or strings ("kg", "R in Medicine", "11.5", "5mg")
          
. . .

4. **Factor**
    - A special class of variables, helpful when working with categorical or ordinal variables
    - Factors assign an order to your variable groups
    - You must assign this class to your variables
    - You can learn more about working with factors in this article: [Wrangling Categorical Data in R](https://peerj.com/preprints/3163/)
      

## Exercise

What is the R class for the following variables?

::: panel-tabset

### var1

::: {.fragment}
```{r}
#| echo: false

var1 <- c(" 7.5", "2", "3.6")

var1

```

:::
<br>

::: {.fragment}
```{r}

class(var1)

```

:::


### var2

::: {.fragment}
```{r}
#| echo: false

var2 <- c("medium", "medium", "low", "high", "low")

var2 <- factor(var2, levels = c("low", "medium", "high"))

var2

```

:::
<br>

::: {.fragment}
```{r}
class(var2)

```

:::

### var3

::: {.fragment}
```{r}
#| echo: false

var3 <- c("50kg", "59kg", "82kg")

var3

```

:::
<br>

::: {.fragment}
```{r}

class(var3)

```

:::
:::

## Exercise

What variable type issues do you notice in our sample data?

![](images/type_clean.PNG){fig-align="center" width=80% height=80%}

```{r}
#| echo: false
#| cache: false
countdown(minutes = 1, font_size = "1em")
```

## Variable Types

- Dates stored as numbers
- Text stored in numeric variables

![](images/type.PNG){fig-align="center"}


## Missing Data

1. Missing data should appear as you expect it to
    - The amount of missingness
    - The variables/cases that data is missing for

. . .

2. Use consistent values to indicate missing responses (Blank, NA, -999)
    - Document your decision
    - The missing values should match your variable type
      - i.e., Don't use "no response" in a numeric variable
      

## Exercise

What missing data issues do you notice in our sample data?

![](images/missing_clean.PNG){fig-align="center" width=75% height=70%}

```{r}
#| echo: false
#| cache: false
countdown(minutes = 1, font_size = "1em")
```
    
## Missing Data

- Unexpected missing data
- Inconsistent missing values used
- Missing values do not match variable type

![](images/missing.PNG){fig-align="center"}


## Error Reduction

The number one way to reduce data errors is to make a plan before you collect data

> Correct data at the source

<br>

. . .

`r fontawesome::fa("check", fill = fa_fill)` Plan the variables you want to collect

. . .

`r fontawesome::fa("check", fill = fa_fill)` Build your data collection/entry tools in a way that follows your plan

. . .

`r fontawesome::fa("check", fill = fa_fill)` Test your data tools before collecting/entering data

. . .

`r fontawesome::fa("check", fill = fa_fill)` Check your data often during data collection/entry


## Plan the variables you want to collect

<br>

:::: {.columns}

::: {.column width="50%"}

**Necessary to plan for**

- Variable name
- Variable label/item wording
- Variable type
- Allowable values/ranges
- Missing values

:::

::: {.column width="50%"}

**Nice to plan for**

- Skip patterns
- Required items
- Variable universe

:::

::::


## Add those variables to a data dictionary

<br>

```{r dictionary}
#| echo: false

library(tidyverse)
library(gt)

dict <- tribble(~var_name, ~label, ~type, ~values, ~missing_values,
                "pat_id", "Patient Identifier",
                "character", "001-030", NA,
                "treatment", "Treatment for UC", "character", "upa; uste; oza", NA,
                "start_date", "Date of start of treatment",
                "date", "YYYY-MM-DD", NA,
                "ethnic", "Ethnicity - hispanic or not hispanic",
                "character", "hispanic; not hispanic", "missing",
                "start_mes", "Mayo endoscopic Score at start of treatment", "numeric", "0-3", "-99")

gt::gt(dict)

```



## Build your tools based on your data dictionary

- Excel
- Qualtrics
- REDCap

- Know the strengths and limitations of your tool
  - Consider things such a privacy (PHI), versioning, validation
  - [Tidy Spreadsheets in Medical Research (70 mins)](https://www.youtube.com/watch?v=9f-hpJbjKZo&t=7s)
  
  
## Build your tools based on your data dictionary

. . . 

<br>

`r fontawesome::fa("check", fill = fa_fill)` Name your variables correctly in your tool  

- Instead of Q1, Q2, Q3 -> id, start_date, treatment

. . .

`r fontawesome::fa("check", fill = fa_fill)` Build items to only accept allowable values

-   Only within specified range (0-50)
-   Only within specified categories ("hispanic", "not hispanic")

. . .

`r fontawesome::fa("check", fill = fa_fill)` Build items to only accept specified variable types

-   Only numeric values
-   Only dates in the YYYY-MM-DD format

## Test your data collection or entry tool

- Collect/enter sample data
  - Are any items missing?
  - Are you getting unexpected values for items?
    - Values out of range
    - Incorrect formats
    - Inconsistent entries
      - "m", "male", "Male", "MALE"
  - Is the skip logic working as expected?
  - Are people able to skip items that they should not be able to skip?
  

## Review your data often during data collection or entry

1. Validate your data based on your expectations
    - `pointblank`
    - `validate`
    - `assertr`
    - `dataquieR`
    - Excellent resource: [Data Validation in R: From Principles to Tools and Packages (80 minutes)](https://www.youtube.com/watch?v=0d1c-8yw6Tk)
    

## `pointblank` report

::: panel-tabset

## Code

```{r}
#| echo: false

library(tidyverse)
  
df_raw <- readxl::read_excel("../data/messy_uc.xlsx", sheet = "Data", skip = 5) |>
  filter(!is.na(pat_id)) |>
  slice_head(n=10) |>
  select(pat_id:dob, start_bp, start_mes)

```

```{r}
#| eval: false

library(pointblank)

# Import my data

df_raw <- readxl::read_excel("data/mydata.csv")

# Check my assumptions

create_agent(df_raw) |>
  rows_distinct(columns = vars(pat_id)) |>
  col_vals_not_null(columns = vars(pat_id)) |>
  col_is_date(columns = vars(start_date)) |>
  col_is_numeric(columns = vars(start_mes)) |>
  col_vals_in_set(columns = vars(treatment), set = c("upa", "uste", "oza")) |>
  col_vals_in_set(columns = vars(ethnic), set = c("hispanic", "not hispanic")) |>
  col_vals_between(columns = vars(start_mes), left = 0, right = 3, na_pass = FALSE) |>
  interrogate()

```

## Report

![](images/pointblank.PNG){fig-align="center" width=80% height=90%}

:::

## Review your data often during data collection

2. Create a codebook to review univariate summary statistics
    - `codebookr`
    - `codebook`
    - `memisc`
    - `sjPlot`
    

## `codebookr` codebook

::: panel-tabset

## Code

```{r}
#| eval: false

library(codebookr)

# Import my data

df_raw <- readxl::read_excel("data/mydata.csv")

# Create my codebook

df_codebook <- codebook(df_raw)

print(df_codebook,"my_codebookr_codebook.docx")

```

## Codebook

![](images/codebook.PNG){fig-align="center" width=85%}


:::


# Data Cleaning Practices

::: {.notes}

So all of those practices we just covered are obviously done in an ideal world, where we have autonomy over how data is collected. But there are obviously still going to be situations where you are handed data that you had no control over the collection/entry process.

Or maybe, even if you did collect your own data, despite your best efforts to collect/enter clean data, you still ended up with data that contains errors

For the remainder of this workshop we will be working through a sample messy dataset to both identify and resolve issues to leave us with a usable tidy dataset that is ready for analysis.

:::

## Scenario

- We have data that originate from an observational study comparing 3 treatments of ulcerative colitis (UC)

- We have an analysis question:
  - Are there differences in change in MES and QOL scores between start and finish, and are the decreases in scores greater for any of the 3 new medications?
  
- In order to answer this question, we have asked a student to extract data from the medical record into Excel

- Along with the spreadsheet, we are provided a data dictionary

- As we start to review the data, we find a sundry of errors that need correction


## Exercise


Take 5 minutes to review the data dictionary and our data.

1. Log in to Posit Cloud and navigate to our project
2. Open the data folder and open the file "messy_uc.xlsx"

:::: {layout="[[45,-10,45], [100]]"}
::: {.column}
![](images/data_file.PNG){width=80%}
:::

::: {.column}
-   When you finish, give us a `r knitr::asis_output("\U1F44D")`

-   If you are having trouble, give us a `r knitr::asis_output("\U270B")`
:::
::::


```{r}
#| echo: false
#| cache: false
countdown(minutes = 5, font_size = "2em")
```

## Import our file

- We are going to use the `read_excel()` function from the `readxl` package

- There are several arguments to consider when using this function
  - path
  - sheet = NULL
  - col_names = TRUE
  - na = " "
  - skip = 0

- type `?read_excel` in your console to see more arguments


## Import our file

::: panel-tabset

## Script

![](images/script2.PNG){width=50%}

## Code

```{r, error = TRUE, eval = FALSE}
 
library(readxl)

# Import our file

df_raw <- readxl::read_excel("data/messy_uc.xlsx",
  sheet = "__", 
  skip = __
)

```


## Data

```{r}
#| echo: false
 
library(readxl)

df_raw <- read_excel("../data/messy_uc.xlsx",
  sheet = "Data", skip = 6
)

```

```{r}
#| echo: false

df_raw |>
  slice(1:10) |>
  select(pat_id:start_mes) |>
  gt::gt()

```

:::

## Exercise (CL1)

<br>

Your turn! Take 3 minutes to import the data.

1. Open "exercises.qmd" in our Posit Cloud project
2. Navigate to ## CL1
3. Update the code and run the code chunk using the green arrow

<br>

[--> Take me to the exercises <--](https://shannonpileggi.github.io/rmedicine-data-cleaning-2023/exercises.html#cl1){target="_blank"}

```{r}
#| echo: false
#| cache: false
countdown(minutes = 3, font_size = "2em")
```



## Review the data

> EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. 
*- R for Data Science*

![](https://media.giphy.com/media/lXu72d4iKwqek/giphy.gif){fig-align="center" width=80%}

::: footer  
Image from giphy.com
:::  


## Review the data

<br>

:::: {.columns}

::: {.column width="40%"}

Get to know your data  

  - How many rows? How many columns?
  - What are the variable types?
  - What are variable values?
  - How much missing data is there?
  - How are variables related?

:::

::: {.column width="10%"}

:::

::: {.column width="50%"}

There are several functions that can be used to explore data

- `dplyr::glimpse()`
- `skimr::skim()`
- `base::summary()`
- `visdat:vis_dat()`
- `summarytools::dfSummary()`
- `DataExplorer::create_report()`
- `Hmisc::describe()`

:::

::::


## `summarytools::dfSummary()`

::: panel-tabset

## Code

```{r}
#| eval: false

library(summarytools)

# Review our data

dfSummary(df_raw)

```


## Output

![](images/exploratory1.PNG){width=50%}![](images/exploratory2.PNG){width=50% height=90%}

:::

## `skimr::skim()`

::: panel-tabset

## Code

```{r}
#| eval: false

library(skimr)

# Review our data

skim(df_raw)

```

## Output

![](images/skimr.PNG){width=90%}

:::


## Exercise (CL2)

Use one or more of these exploratory packages to review your data. What fixes do you see that need to happen?

:::: {.columns}

::: {.column width="40%"}
- `dplyr::glimpse()`
- `skimr::skim()`
- `base::summary()`
- `visdat:vis_dat()`

:::

::: {.column width="40%"}
- `summarytools::dfSummary()`
- `DataExplorer::create_report()`
- `Hmisc::describe()`
:::

::::

<br>

[--> Take me to the exercises <--](https://shannonpileggi.github.io/rmedicine-data-cleaning-2023/exercises.html#cl2){target="_blank"}

```{r}
#| echo: false
#| cache: false
countdown(minutes = 5, font_size = "2em")
```



